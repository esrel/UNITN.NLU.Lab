{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dependency Grammars with NLTK\n",
    "\n",
    "- Evgeny A. Stepanov\n",
    "- stapanov.evgeny.a@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Understanding:\n",
    "    - Dependency Relations and Grammars\n",
    "    - Probabilistic Dependency Grammars\n",
    "    - Projective and Non-Projective Parses\n",
    "    - Transition-based Dependency Parsing\n",
    "\n",
    "- Learning how to:\n",
    "    - define dependency grammar in NLTK\n",
    "    - identify a syntactic relation between Head and Dependent\n",
    "    - parse with dependency grammar\n",
    "    - evaluate dependency parser\n",
    "    - use dependency parser of spacy and stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended Reading\n",
    "- Dan Jurafsky and James H. Martin. [__Speech and Language Processing__ (SLP)](https://web.stanford.edu/~jurafsky/slp3/) (3rd ed. draft)\n",
    "- Steven Bird, Ewan Klein, and Edward Loper. [__Natural Language Processing with Python__ (NLTK)](https://www.nltk.org/book/)\n",
    "- Kübler, McDonald, and Nivre (2009) Dependency Parsing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covered Material\n",
    "- SLP\n",
    "    - [Chapter 14: Dependency Parsing](https://web.stanford.edu/~jurafsky/slp3/14.pdf) \n",
    "- NLTK \n",
    "    - [Chapter 8: Analyzing Sentence Structure](https://www.nltk.org/book/ch08.html)\n",
    "- Kübler, McDonald, and Nivre (2009) Dependency Parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Requirements\n",
    "\n",
    "- [NLTK](https://www.nltk.org/)\n",
    "    - run `pip install nltk`\n",
    "- [spaCy](https://spacy.io/)\n",
    "    - run `pip install spacy`\n",
    "    - run `python -m spacy download en_core_web_sm` to install English models\n",
    "- [stanza](https://stanfordnlp.github.io/stanza/) for Stanford Parser\n",
    "    - run `pip install stanza`\n",
    "    - run `stanza.download('en')` to intall English models\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Dependency Grammars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Unlike Constituency (Phrase Structure) Grammar that addresses how words and sequences of words combine to form constituents, Dependency Grammar addresses on how words relate to each other. \n",
    "\n",
    "Dependency Grammar assumes that syntactic structure consists of words linked by binary, asymmetrical relations called __dependency relations__. A dependency relation is a binary asymmetric relation that holds between a syntactically subordinate word, called the __dependent__, and another word on which it depends, called the __head__.\n",
    "\n",
    "The __head of a sentence__ is usually taken to be the tensed verb, and every other word is either dependent on the sentence head, or connects to it through a path of dependencies. Thus, a dependency parse is a __directed graph__, where the nodes are the lexical items (words) and the arcs represent dependency relations from heads to dependents. \n",
    "\n",
    "A __typed dependency structure__ contains of the __labeled__ arcs are drawn from a fixed inventory of grammatical relations, that also includes a __root node__ that explicitly marks the root of the tree, the head of the entire structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1. Dependency Relation Types\n",
    "\n",
    "Universal Dependency set defines the following __core__ relations (from Jurafsky & Martin). \n",
    "\n",
    "(See https://universaldependencies.org/u/dep/index.html for the full set.)\n",
    "\n",
    "| __Clausal Argument Relations__ | Description | Example |\n",
    "|:-------------------------------|:------------|:--------\n",
    "| NSUBJ  | Nominal subject       | __We__ booked her the cheapest morning flight to Miami.\n",
    "| DOBJ   | Direct object         | We booked her the cheapest morning __flight__ to Miami.\n",
    "| IOBJ   | Indirect object       | We booked __her__ the cheapest morning flight to Miami.\n",
    "| CCOMP  | Clausal complement    |\n",
    "| XCOMP  | Open clausal complement (subject of clause is out of its span) \n",
    "| __Nominal Modifier Relations__ ||\n",
    "| NMOD   | Nominal modifier      | We booked her the cheapest __morning__ flight to Miami.\n",
    "| AMOD   | Adjectival modifier   | We booked her the __cheapest__ morning flight to Miami.\n",
    "| NUMMOD | Numeric modifier\n",
    "| APPOS  | Appositional modifier\n",
    "| DET    | Determiner            | We booked her __the__ cheapest morning flight to Miami.\n",
    "| CASE   | Prepositions, postpositions and other case markers | We booked her the cheapest morning flight __to__ Miami.\n",
    "| __Other Notable Relations__ | \n",
    "| CONJ   | Conjunct\n",
    "| CC     | Coordinating conjunction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.2. Defining Dependency Grammar in NLTK\n",
    "\n",
    "Similar to Phrase Structure Grammar, Dependecy Grammar is defined as a list of production rules.\n",
    "\n",
    "Below is an example grammar that defines only __bare__ dependency relations without specifying their types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency grammar with 7 productions\n",
      "  'saw' -> 'i'\n",
      "  'saw' -> 'man'\n",
      "  'saw' -> 'with'\n",
      "  'man' -> 'the'\n",
      "  'man' -> 'with'\n",
      "  'with' -> 'telescope'\n",
      "  'telescope' -> 'a'\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# for sentence \"i saw the man with the telescope\"\n",
    "# only string input is accepted\n",
    "rules = \"\"\"\n",
    "    'saw' -> 'i' | 'man' | 'with'\n",
    "    'man' -> 'the' | 'with'\n",
    "    'with' -> 'telescope'\n",
    "    'telescope' -> 'a'\n",
    "\"\"\"\n",
    "\n",
    "toy_grammar = nltk.DependencyGrammar.fromstring(rules)\n",
    "\n",
    "print(toy_grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Unlike Phrase Structure Grammar, \n",
    "\n",
    "- there is no start symbol (thus, no method to access it)\n",
    "- there is no method to access productions, but it is still possible using the attribute\n",
    "\n",
    "    - `grammar._productions`\n",
    "\n",
    "- there is a method to check if grammar contains a production\n",
    "\n",
    "    - `grammar.contains(head, mod)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Dependency Production__ has 2 attributes:\n",
    "\n",
    "- `_lhs` (left-hand side) -- head\n",
    "- `_rhs` (right-hand side) -- modifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['saw' -> 'i', 'saw' -> 'man', 'saw' -> 'with', 'man' -> 'the', 'man' -> 'with', 'with' -> 'telescope', 'telescope' -> 'a']\n",
      "saw ('i',)\n",
      "saw ('man',)\n",
      "saw ('with',)\n",
      "man ('the',)\n",
      "man ('with',)\n",
      "with ('telescope',)\n",
      "telescope ('a',)\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(toy_grammar._productions)\n",
    "\n",
    "for production in toy_grammar._productions:\n",
    "    print(production._lhs, production._rhs)\n",
    "\n",
    "print(toy_grammar.contains('man', 'the'))  # True\n",
    "print(toy_grammar.contains('the', 'man'))  # False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### How to Identify a Syntactic Relation between Head and Dependent\n",
    "(From Kübler et al. & NLTK Book)\n",
    "\n",
    "Here is a list of some of the more common criteria that have been proposed for identifying a syntactic relation between a head __H__ and a dependent __D__ in a linguistic construction __C__:\n",
    "\n",
    "1. __H__ determines the syntactic category of __C__ and can often replace __C__.\n",
    "2. __H__ determines the semantic category of __C__; __D__ gives semantic specification.\n",
    "3. __H__ is obligatory; __D__ may be optional.\n",
    "4. __H__ selects __D__ and determines whether __D__ is obligatory or optional. \n",
    "5. The form of __D__ depends on __H__ (agreement or government).\n",
    "6. The linear position of __D__ is specified with reference to __H__ .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example__:\n",
    "\n",
    "_I prefer a morning flight_\n",
    "\n",
    "- **C**: _morning flight_\n",
    "- **H**: _flight_\n",
    "    - determines syntactic category: whole construction is nominal\n",
    "    - determines semantic category\n",
    "    - comes after _morning_ (English is head final)\n",
    "- **D**: _morning_\n",
    "    - optional w.r.t. _flight_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.3. Parsing with Dependency Grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since Dependency Graphs can be projective and non-projective (allow crossing dependencies), there are __projective__ and __non-projective__ parsers. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.3.1. Projective Dependency Parser (Rule-based)\n",
    "\n",
    "> A projective, rule-based, dependency parser. A [`ProjectiveDependencyParser`](http://www.nltk.org/api/nltk.parse.html#module-nltk.parse.projectivedependencyparser) is created with a `DependencyGrammar`, a set of productions specifying word-to-word dependency relations. The `parse()` method will then return the set of all parses, in tree representation, for a given input sequence of tokens. Each parse must meet the requirements of the both the grammar and the projectivity constraint which specifies that the branches of the dependency tree are not allowed to cross.\n",
    "\n",
    "`parse()` method returns iterator over [`Tree`](http://www.nltk.org/_modules/nltk/tree.html) objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(saw i (man the (with (telescope a))))\n",
      "The ROOT is 'saw'\n",
      "(saw i (man the) (with (telescope a)))\n",
      "The ROOT is 'saw'\n"
     ]
    }
   ],
   "source": [
    "parser = nltk.ProjectiveDependencyParser(toy_grammar)\n",
    "\n",
    "sent = \"i saw the man with a telescope\"\n",
    "\n",
    "for tree in parser.parse(sent.split()):\n",
    "    print(tree)\n",
    "    # print ROOT node\n",
    "    print(\"The ROOT is '{}'\".format(tree.label()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.3.2. Non-Projective Dependency Parser (Rule-Based)\n",
    "\n",
    "> A non-projective, rule-based, dependency parser. This parser will return the set of all possible non-projective parses based on the word-to-word relations defined in the parser’s dependency grammar, and will allow the branches of the parse tree to cross in order to capture a variety of linguistic phenomena that a projective parser will not.\n",
    "\n",
    "`parse()` method returns iterator over [`DependencyGraph`](https://www.nltk.org/api/nltk.parse.html#nltk.parse.dependencygraph.DependencyGraph) objects. \n",
    "\n",
    "`tree()` method of the `DependencyGraph` object builds a dependency tree using the NLTK Tree constructor, starting with the `root` node and omitting labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(saw i (man the (with (telescope a))))\n",
      "The ROOT is 'saw'\n",
      "(saw i (man the) (with (telescope a)))\n",
      "The ROOT is 'saw'\n"
     ]
    }
   ],
   "source": [
    "np_parser = nltk.NonprojectiveDependencyParser(toy_grammar)\n",
    "\n",
    "for graph in np_parser.parse(sent.split()):\n",
    "    graph.tree().pprint()\n",
    "    print(\"The ROOT is '{}'\".format(graph.root['word']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since the sentence is ambiguous, similar to Phrase Structure Grammar, our Dependency Grammar yields 2 parses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.3.3. Accessing the Graph\n",
    "\n",
    "- `DependencyGraph` object has 2 attrubutes\n",
    "    - nodes (of `defaultdict` type)\n",
    "    - root (of `dict` type), which is also a node\n",
    "\n",
    "- Each node in a graph is represented as a dict that defines its:\n",
    "    - address (sentence index starting from 1) -- required\n",
    "    - word (string form) -- required\n",
    "    - head (address)\n",
    "    - deps (dependents)\n",
    "    - rel (dependency relation to head)\n",
    "\n",
    "Thus, we can print the graph as a list of tokens with their attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 saw\n",
      "2 saw\n"
     ]
    }
   ],
   "source": [
    "# printing root address and word\n",
    "for graph in np_parser.parse(sent.split()): \n",
    "    print(graph.root['address'], graph.root['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\ti:\t[]\n",
      "2\tsaw:\t[1, 4]\n",
      "3\tthe:\t[]\n",
      "4\tman:\t[3, 5]\n",
      "5\twith:\t[7]\n",
      "6\ta:\t[]\n",
      "7\ttelescope:\t[6]\n"
     ]
    }
   ],
   "source": [
    "# printing all the nodes with dependent positions\n",
    "for graph in np_parser.parse(sent.split()):    \n",
    "    # sorting is required since graph starts from root, which is not the first token\n",
    "    for _, node in sorted(graph.nodes.items()):\n",
    "        if node['word'] is not None:\n",
    "            print('{address}\\t{word}:\\t{dependents}'.format(dependents=node['deps'][''], **node))\n",
    "    break  # just to print 1 graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is also possible to convert the graph into other supported formats, such as CoNLL using `to_conll(style)` method, where [style](https://www.nltk.org/api/nltk.parse.html#nltk.parse.dependencygraph.DependencyGraph) is either 3, 4, or 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\tNone\tNone\n",
      "saw\tNone\tNone\n",
      "the\tNone\tNone\n",
      "man\tNone\tNone\n",
      "with\tNone\tNone\n",
      "a\tNone\tNone\n",
      "telescope\tNone\tNone\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for graph in np_parser.parse(sent.split()):    \n",
    "    print(graph.to_conll(3))\n",
    "    break  # just to print 1 graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise\n",
    "\n",
    "- Define grammar that covers the following sentences.\n",
    "\n",
    "    - show flights from new york to los angeles\n",
    "    - list flights from new york to los angeles\n",
    "    - show flights from new york\n",
    "    - list flights to los angeles\n",
    "    - list flights\n",
    "    \n",
    "- Use one of the parsers to parse the sentences (i.e. test your grammar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Probabilistic Dependency Grammars & Parsing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Similar to CFGs, we can learn dependency grammar from data using treebanks.\n",
    "NLTK provides `ProbabilisticProjectiveDependencyParser` that returns the most probable projective parse derived from the probabilistic dependency grammar derived from the `train()` method. \n",
    "\n",
    "> The probabilistic model is an implementation of Eisner's (1996) Model C, which conditions on head-word, head-tag, child-word, and child-tag. The decoding uses a bottom-up chart-based span concatenation algorithm that's identical to the one utilized by the rule-based projective parser.\n",
    "\n",
    "Without going into details, that is an example of Dynamic Programming approach to Dependency Parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package dependency_treebank to\n",
      "[nltk_data]     /Users/eas/nltk_data...\n",
      "[nltk_data]   Package dependency_treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# downloading treebank\n",
    "nltk.download('dependency_treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tPierre\tPierre\tNNP\tNNP\t\t2\t\t_\t_\n",
      "2\tVinken\tVinken\tNNP\tNNP\t\t8\t\t_\t_\n",
      "3\t,\t,\t,\t,\t\t2\t\t_\t_\n",
      "4\t61\t61\tCD\tCD\t\t5\t\t_\t_\n",
      "5\tyears\tyears\tNNS\tNNS\t\t6\t\t_\t_\n",
      "6\told\told\tJJ\tJJ\t\t2\t\t_\t_\n",
      "7\t,\t,\t,\t,\t\t2\t\t_\t_\n",
      "8\twill\twill\tMD\tMD\t\t0\t\t_\t_\n",
      "9\tjoin\tjoin\tVB\tVB\t\t8\t\t_\t_\n",
      "10\tthe\tthe\tDT\tDT\t\t11\t\t_\t_\n",
      "11\tboard\tboard\tNN\tNN\t\t9\t\t_\t_\n",
      "12\tas\tas\tIN\tIN\t\t9\t\t_\t_\n",
      "13\ta\ta\tDT\tDT\t\t15\t\t_\t_\n",
      "14\tnonexecutive\tnonexecutive\tJJ\tJJ\t\t15\t\t_\t_\n",
      "15\tdirector\tdirector\tNN\tNN\t\t12\t\t_\t_\n",
      "16\tNov.\tNov.\tNNP\tNNP\t\t9\t\t_\t_\n",
      "17\t29\t29\tCD\tCD\t\t16\t\t_\t_\n",
      "18\t.\t.\t.\t.\t\t8\t\t_\t_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example from NLTK\n",
    "from nltk.parse.dependencygraph import DependencyGraph\n",
    "from nltk.parse import ProbabilisticProjectiveDependencyParser\n",
    "from nltk.corpus import dependency_treebank\n",
    "\n",
    "# print dependency graph in CoNLL format\n",
    "print(dependency_treebank.parsed_sents()[0].to_conll(10))\n",
    "\n",
    "ppdp = ProbabilisticProjectiveDependencyParser()\n",
    "\n",
    "# train parser on graphs\n",
    "ppdp.train(dependency_treebank.parsed_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(fell (stock (of (price the)) the))\n",
      "(fell (stock (of (price the) the)))\n",
      "(fell (stock (of the price) the))\n",
      "(fell (stock (of the price the)))\n",
      "(fell (stock the (of price) the))\n",
      "(fell (stock the (of price the)))\n"
     ]
    }
   ],
   "source": [
    "# parse the sentence\n",
    "parse = ppdp.parse(['the', 'price', 'of', 'the', 'stock', 'fell'])\n",
    "\n",
    "# returns set of trees ordered by probability score\n",
    "for tree in parse:\n",
    "    print(tree)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "Write a function that given a dependency graph, for each token (word), produces list of words from it to ROOT.\n",
    "\n",
    "(Construct normal `dict` for simplicity first.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function DependencyGraph.__init__.<locals>.<lambda> at 0x7f8d574d3670>,\n",
      "            {0: {'address': 0,\n",
      "                 'ctag': 'TOP',\n",
      "                 'deps': defaultdict(<class 'list'>, {'ROOT': [8]}),\n",
      "                 'feats': None,\n",
      "                 'head': None,\n",
      "                 'lemma': None,\n",
      "                 'rel': None,\n",
      "                 'tag': 'TOP',\n",
      "                 'word': None},\n",
      "             1: {'address': 1,\n",
      "                 'ctag': 'NNP',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '',\n",
      "                 'head': 2,\n",
      "                 'lemma': 'Pierre',\n",
      "                 'rel': '',\n",
      "                 'tag': 'NNP',\n",
      "                 'word': 'Pierre'},\n",
      "             2: {'address': 2,\n",
      "                 'ctag': 'NNP',\n",
      "                 'deps': defaultdict(<class 'list'>, {'': [1, 3, 6, 7]}),\n",
      "                 'feats': '',\n",
      "                 'head': 8,\n",
      "                 'lemma': 'Vinken',\n",
      "                 'rel': '',\n",
      "                 'tag': 'NNP',\n",
      "                 'word': 'Vinken'},\n",
      "             3: {'address': 3,\n",
      "                 'ctag': ',',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '',\n",
      "                 'head': 2,\n",
      "                 'lemma': ',',\n",
      "                 'rel': '',\n",
      "                 'tag': ',',\n",
      "                 'word': ','},\n",
      "             4: {'address': 4,\n",
      "                 'ctag': 'CD',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '',\n",
      "                 'head': 5,\n",
      "                 'lemma': '61',\n",
      "                 'rel': '',\n",
      "                 'tag': 'CD',\n",
      "                 'word': '61'},\n",
      "             5: {'address': 5,\n",
      "                 'ctag': 'NNS',\n",
      "                 'deps': defaultdict(<class 'list'>, {'': [4]}),\n",
      "                 'feats': '',\n",
      "                 'head': 6,\n",
      "                 'lemma': 'years',\n",
      "                 'rel': '',\n",
      "                 'tag': 'NNS',\n",
      "                 'word': 'years'},\n",
      "             6: {'address': 6,\n",
      "                 'ctag': 'JJ',\n",
      "                 'deps': defaultdict(<class 'list'>, {'': [5]}),\n",
      "                 'feats': '',\n",
      "                 'head': 2,\n",
      "                 'lemma': 'old',\n",
      "                 'rel': '',\n",
      "                 'tag': 'JJ',\n",
      "                 'word': 'old'},\n",
      "             7: {'address': 7,\n",
      "                 'ctag': ',',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '',\n",
      "                 'head': 2,\n",
      "                 'lemma': ',',\n",
      "                 'rel': '',\n",
      "                 'tag': ',',\n",
      "                 'word': ','},\n",
      "             8: {'address': 8,\n",
      "                 'ctag': 'MD',\n",
      "                 'deps': defaultdict(<class 'list'>, {'': [2, 9, 18]}),\n",
      "                 'feats': '',\n",
      "                 'head': 0,\n",
      "                 'lemma': 'will',\n",
      "                 'rel': '',\n",
      "                 'tag': 'MD',\n",
      "                 'word': 'will'},\n",
      "             9: {'address': 9,\n",
      "                 'ctag': 'VB',\n",
      "                 'deps': defaultdict(<class 'list'>, {'': [11, 12, 16]}),\n",
      "                 'feats': '',\n",
      "                 'head': 8,\n",
      "                 'lemma': 'join',\n",
      "                 'rel': '',\n",
      "                 'tag': 'VB',\n",
      "                 'word': 'join'},\n",
      "             10: {'address': 10,\n",
      "                  'ctag': 'DT',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '',\n",
      "                  'head': 11,\n",
      "                  'lemma': 'the',\n",
      "                  'rel': '',\n",
      "                  'tag': 'DT',\n",
      "                  'word': 'the'},\n",
      "             11: {'address': 11,\n",
      "                  'ctag': 'NN',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [10]}),\n",
      "                  'feats': '',\n",
      "                  'head': 9,\n",
      "                  'lemma': 'board',\n",
      "                  'rel': '',\n",
      "                  'tag': 'NN',\n",
      "                  'word': 'board'},\n",
      "             12: {'address': 12,\n",
      "                  'ctag': 'IN',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [15]}),\n",
      "                  'feats': '',\n",
      "                  'head': 9,\n",
      "                  'lemma': 'as',\n",
      "                  'rel': '',\n",
      "                  'tag': 'IN',\n",
      "                  'word': 'as'},\n",
      "             13: {'address': 13,\n",
      "                  'ctag': 'DT',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '',\n",
      "                  'head': 15,\n",
      "                  'lemma': 'a',\n",
      "                  'rel': '',\n",
      "                  'tag': 'DT',\n",
      "                  'word': 'a'},\n",
      "             14: {'address': 14,\n",
      "                  'ctag': 'JJ',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '',\n",
      "                  'head': 15,\n",
      "                  'lemma': 'nonexecutive',\n",
      "                  'rel': '',\n",
      "                  'tag': 'JJ',\n",
      "                  'word': 'nonexecutive'},\n",
      "             15: {'address': 15,\n",
      "                  'ctag': 'NN',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [13, 14]}),\n",
      "                  'feats': '',\n",
      "                  'head': 12,\n",
      "                  'lemma': 'director',\n",
      "                  'rel': '',\n",
      "                  'tag': 'NN',\n",
      "                  'word': 'director'},\n",
      "             16: {'address': 16,\n",
      "                  'ctag': 'NNP',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [17]}),\n",
      "                  'feats': '',\n",
      "                  'head': 9,\n",
      "                  'lemma': 'Nov.',\n",
      "                  'rel': '',\n",
      "                  'tag': 'NNP',\n",
      "                  'word': 'Nov.'},\n",
      "             17: {'address': 17,\n",
      "                  'ctag': 'CD',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '',\n",
      "                  'head': 16,\n",
      "                  'lemma': '29',\n",
      "                  'rel': '',\n",
      "                  'tag': 'CD',\n",
      "                  'word': '29'},\n",
      "             18: {'address': 18,\n",
      "                  'ctag': '.',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '',\n",
      "                  'head': 8,\n",
      "                  'lemma': '.',\n",
      "                  'rel': '',\n",
      "                  'tag': '.',\n",
      "                  'word': '.'}})\n"
     ]
    }
   ],
   "source": [
    "graph = dependency_treebank.parsed_sents()[0]\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Transition-Based Dependency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are several methods for data-driven dependency parsing\n",
    "- Dynamic Programming-based (e.g. `ProbabilisticProjectiveDependencyParser`)\n",
    "- Graph-based (e.g. [Minimum Spanning Tree Parser](https://www.seas.upenn.edu/~strctlrn/MSTParser/MSTParser.html))\n",
    "- Transition-Based Dependency Parsing (e.g. NLTK's interface to [MaltParser](https://www.nltk.org/_modules/nltk/parse/malt.html))\n",
    "\n",
    "Transition-based parsing (or \"deterministic dependency parsing\") proved to be very effective and is the State-of-the-Art apprach (with neural twist)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(from Jurafsky & Martin)\n",
    "\n",
    "In transition-based parsing there is:\n",
    "- a **stack** on which we build the parse\n",
    "- a **buffer** of tokens to be parsed\n",
    "- a **parser** which takes actions on the parse via a predictor called an **oracle**\n",
    "\n",
    "The parser walks through the sentence left-to-right, successively shifting items from the buffer onto the stack. At each time point we examine the top two elements on the stack, and the oracle makes a decision about what transition to apply to build the parse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arc-Standard Transition System (there are alternatives, i.e. Arc Eager) defines 3 transition operators that will operate on the top two elements of the stack:\n",
    "\n",
    "- __LEFTARC__: \n",
    "    - Assert a head-dependent relation between the word at the top of the stack and the word directly beneath it;\n",
    "    - Remove the lower word from the stack.\n",
    "- __RIGHTARC__: \n",
    "    - Assert a head-dependent relation between the second word on the stack and the word at the top; \n",
    "    - Remove the word at the top of the stack;\n",
    "- __SHIFT__: \n",
    "    - Remove the word from the front of the input buffer and push it onto the stack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally fits into Machine Learning framework where \n",
    "- configuration (buffer & stack) are features, \n",
    "- operations are labels\n",
    "- oracle is a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transition Parser in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of training examples : 100\n",
      " Number of valid (projective) examples : 100\n",
      "[LibSVM]..*.*\n",
      "optimization finished, #iter = 3898\n",
      "obj = -110.015582, rho = 0.678265\n",
      "nSV = 1564, nBSV = 39\n",
      "Total nSV = 1564\n",
      "..*..*\n",
      "optimization finished, #iter = 4039\n",
      "obj = -115.191632, rho = 0.652690\n",
      "nSV = 1603, nBSV = 44\n",
      "Total nSV = 1603\n",
      "..*..*\n",
      "optimization finished, #iter = 4165\n",
      "obj = -111.789288, rho = 0.673310\n",
      "nSV = 1605, nBSV = 39\n",
      "Total nSV = 1605\n",
      "..*..*\n",
      "optimization finished, #iter = 4074\n",
      "obj = -113.977941, rho = 0.662650\n",
      "nSV = 1603, nBSV = 43\n",
      "Total nSV = 1603\n",
      "..*..*\n",
      "optimization finished, #iter = 4075\n",
      "obj = -114.353287, rho = 0.667683\n",
      "nSV = 1602, nBSV = 35\n",
      "Total nSV = 1602\n",
      "...*.*\n",
      "optimization finished, #iter = 4902\n",
      "obj = -131.080714, rho = -0.664586\n",
      "nSV = 1856, nBSV = 50\n",
      "..*..*\n",
      "optimization finished, #iter = 4172\n",
      "obj = -183.176717, rho = 0.434718\n",
      "nSV = 1756, nBSV = 166\n",
      "Total nSV = 1756\n",
      "..*..*\n",
      "optimization finished, #iter = 4123\n",
      "obj = -179.538914, rho = 0.451442\n",
      "nSV = 1736, nBSV = 152\n",
      "Total nSV = 1736\n",
      "..*..*\n",
      "optimization finished, #iter = 4161\n",
      "obj = -182.481892, rho = 0.445284\n",
      "nSV = 1759, nBSV = 164\n",
      "Total nSV = 1759\n",
      "..*..*\n",
      "optimization finished, #iter = 4232\n",
      "obj = -184.137786, rho = 0.430717\n",
      "nSV = 1759, nBSV = 155\n",
      "Total nSV = 1759\n",
      "..*..*\n",
      "optimization finished, #iter = 4191\n",
      "obj = -177.825289, rho = 0.445493\n",
      "nSV = 1757, nBSV = 155\n",
      "Total nSV = 1757\n",
      "...*..*\n",
      "optimization finished, #iter = 5001\n",
      "obj = -216.029956, rho = -0.387953\n",
      "nSV = 2058, nBSV = 215\n",
      ".*.*\n",
      "optimization finished, #iter = 2589\n",
      "obj = -75.190086, rho = -0.240632\n",
      "nSV = 1195, nBSV = 22\n",
      "Total nSV = 1195\n",
      ".*.*\n",
      "optimization finished, #iter = 2593\n",
      "obj = -70.724095, rho = -0.230451\n",
      "nSV = 1189, nBSV = 19\n",
      "Total nSV = 1189\n",
      ".*.*\n",
      "optimization finished, #iter = 2579\n",
      "obj = -73.904102, rho = -0.250471\n",
      "nSV = 1186, nBSV = 19\n",
      "Total nSV = 1186\n",
      ".*.*\n",
      "optimization finished, #iter = 2592\n",
      "obj = -74.530315, rho = -0.250004\n",
      "nSV = 1190, nBSV = 21\n",
      "Total nSV = 1190\n",
      ".*.*\n",
      "optimization finished, #iter = 2646\n",
      "obj = -73.375057, rho = -0.226262\n",
      "nSV = 1210, nBSV = 22\n",
      "Total nSV = 1210\n",
      "..*.*\n",
      "optimization finished, #iter = 3081\n",
      "obj = -83.670783, rho = 0.240401\n",
      "nSV = 1384, nBSV = 27\n",
      "Total nSV = 3454\n",
      "<nltk.parse.transitionparser.TransitionParser object at 0x7f8d97122dc0>\n"
     ]
    }
   ],
   "source": [
    "from nltk.parse.transitionparser import TransitionParser\n",
    "\n",
    "tp = TransitionParser('arc-standard')\n",
    "tp.train(dependency_treebank.parsed_sents()[:100], 'tp.model')\n",
    "print(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "defaultdict(<function DependencyGraph.__init__.<locals>.<lambda> at 0x7f8d87aa1550>,\n",
      "            {0: {'address': 0,\n",
      "                 'ctag': 'TOP',\n",
      "                 'deps': defaultdict(<class 'list'>, {'ROOT': [5]}),\n",
      "                 'feats': None,\n",
      "                 'head': 0,\n",
      "                 'lemma': None,\n",
      "                 'rel': '',\n",
      "                 'tag': 'TOP',\n",
      "                 'word': None},\n",
      "             1: {'address': 1,\n",
      "                 'ctag': 'DT',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '',\n",
      "                 'head': 4,\n",
      "                 'lemma': 'A',\n",
      "                 'rel': '',\n",
      "                 'tag': 'DT',\n",
      "                 'word': 'A'},\n",
      "             2: {'address': 2,\n",
      "                 'ctag': 'NNP',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '',\n",
      "                 'head': 4,\n",
      "                 'lemma': 'White',\n",
      "                 'rel': '',\n",
      "                 'tag': 'NNP',\n",
      "                 'word': 'White'},\n",
      "             3: {'address': 3,\n",
      "                 'ctag': 'NNP',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '',\n",
      "                 'head': 4,\n",
      "                 'lemma': 'House',\n",
      "                 'rel': '',\n",
      "                 'tag': 'NNP',\n",
      "                 'word': 'House'},\n",
      "             4: {'address': 4,\n",
      "                 'ctag': 'NN',\n",
      "                 'deps': defaultdict(<class 'list'>, {'': [1, 2, 3]}),\n",
      "                 'feats': '',\n",
      "                 'head': 5,\n",
      "                 'lemma': 'spokesman',\n",
      "                 'rel': '',\n",
      "                 'tag': 'NN',\n",
      "                 'word': 'spokesman'},\n",
      "             5: {'address': 5,\n",
      "                 'ctag': 'VBD',\n",
      "                 'deps': defaultdict(<class 'list'>, {'': [4, 7, 8, 31]}),\n",
      "                 'feats': '',\n",
      "                 'head': 0,\n",
      "                 'lemma': 'said',\n",
      "                 'rel': '',\n",
      "                 'tag': 'VBD',\n",
      "                 'word': 'said'},\n",
      "             6: {'address': 6,\n",
      "                 'ctag': 'JJ',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '',\n",
      "                 'head': 7,\n",
      "                 'lemma': 'last',\n",
      "                 'rel': '',\n",
      "                 'tag': 'JJ',\n",
      "                 'word': 'last'},\n",
      "             7: {'address': 7,\n",
      "                 'ctag': 'NN',\n",
      "                 'deps': defaultdict(<class 'list'>, {'': [6]}),\n",
      "                 'feats': '',\n",
      "                 'head': 5,\n",
      "                 'lemma': 'week',\n",
      "                 'rel': '',\n",
      "                 'tag': 'NN',\n",
      "                 'word': 'week'},\n",
      "             8: {'address': 8,\n",
      "                 'ctag': 'IN',\n",
      "                 'deps': defaultdict(<class 'list'>, {'': [11]}),\n",
      "                 'feats': '',\n",
      "                 'head': 7,\n",
      "                 'lemma': 'that',\n",
      "                 'rel': '',\n",
      "                 'tag': 'IN',\n",
      "                 'word': 'that'},\n",
      "             9: {'address': 9,\n",
      "                 'ctag': 'DT',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '',\n",
      "                 'head': 10,\n",
      "                 'lemma': 'the',\n",
      "                 'rel': '',\n",
      "                 'tag': 'DT',\n",
      "                 'word': 'the'},\n",
      "             10: {'address': 10,\n",
      "                  'ctag': 'NN',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [9]}),\n",
      "                  'feats': '',\n",
      "                  'head': 11,\n",
      "                  'lemma': 'president',\n",
      "                  'rel': '',\n",
      "                  'tag': 'NN',\n",
      "                  'word': 'president'},\n",
      "             11: {'address': 11,\n",
      "                  'ctag': 'VBZ',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [10, 12]}),\n",
      "                  'feats': '',\n",
      "                  'head': 8,\n",
      "                  'lemma': 'is',\n",
      "                  'rel': '',\n",
      "                  'tag': 'VBZ',\n",
      "                  'word': 'is'},\n",
      "             12: {'address': 12,\n",
      "                  'ctag': 'VBG',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [13]}),\n",
      "                  'feats': '',\n",
      "                  'head': 11,\n",
      "                  'lemma': 'considering',\n",
      "                  'rel': '',\n",
      "                  'tag': 'VBG',\n",
      "                  'word': 'considering'},\n",
      "             13: {'address': 13,\n",
      "                  'ctag': 'VBG',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [14, 27]}),\n",
      "                  'feats': '',\n",
      "                  'head': 12,\n",
      "                  'lemma': 'declaring',\n",
      "                  'rel': '',\n",
      "                  'tag': 'VBG',\n",
      "                  'word': 'declaring'},\n",
      "             14: {'address': 14,\n",
      "                  'ctag': 'IN',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [18]}),\n",
      "                  'feats': '',\n",
      "                  'head': 13,\n",
      "                  'lemma': 'that',\n",
      "                  'rel': '',\n",
      "                  'tag': 'IN',\n",
      "                  'word': 'that'},\n",
      "             15: {'address': 15,\n",
      "                  'ctag': 'DT',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '',\n",
      "                  'head': 16,\n",
      "                  'lemma': 'the',\n",
      "                  'rel': '',\n",
      "                  'tag': 'DT',\n",
      "                  'word': 'the'},\n",
      "             16: {'address': 16,\n",
      "                  'ctag': 'NNP',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [15]}),\n",
      "                  'feats': '',\n",
      "                  'head': 18,\n",
      "                  'lemma': 'Constitution',\n",
      "                  'rel': '',\n",
      "                  'tag': 'NNP',\n",
      "                  'word': 'Constitution'},\n",
      "             17: {'address': 17,\n",
      "                  'ctag': 'RB',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '',\n",
      "                  'head': 18,\n",
      "                  'lemma': 'implicitly',\n",
      "                  'rel': '',\n",
      "                  'tag': 'RB',\n",
      "                  'word': 'implicitly'},\n",
      "             18: {'address': 18,\n",
      "                  'ctag': 'VBZ',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [16, 17, 19, 21]}),\n",
      "                  'feats': '',\n",
      "                  'head': 14,\n",
      "                  'lemma': 'gives',\n",
      "                  'rel': '',\n",
      "                  'tag': 'VBZ',\n",
      "                  'word': 'gives'},\n",
      "             19: {'address': 19,\n",
      "                  'ctag': 'PRP',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '',\n",
      "                  'head': 21,\n",
      "                  'lemma': 'him',\n",
      "                  'rel': '',\n",
      "                  'tag': 'PRP',\n",
      "                  'word': 'him'},\n",
      "             20: {'address': 20,\n",
      "                  'ctag': 'DT',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '',\n",
      "                  'head': 21,\n",
      "                  'lemma': 'the',\n",
      "                  'rel': '',\n",
      "                  'tag': 'DT',\n",
      "                  'word': 'the'},\n",
      "             21: {'address': 21,\n",
      "                  'ctag': 'NN',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [20, 22]}),\n",
      "                  'feats': '',\n",
      "                  'head': 18,\n",
      "                  'lemma': 'authority',\n",
      "                  'rel': '',\n",
      "                  'tag': 'NN',\n",
      "                  'word': 'authority'},\n",
      "             22: {'address': 22,\n",
      "                  'ctag': 'IN',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [25]}),\n",
      "                  'feats': '',\n",
      "                  'head': 21,\n",
      "                  'lemma': 'for',\n",
      "                  'rel': '',\n",
      "                  'tag': 'IN',\n",
      "                  'word': 'for'},\n",
      "             23: {'address': 23,\n",
      "                  'ctag': 'DT',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '',\n",
      "                  'head': 25,\n",
      "                  'lemma': 'a',\n",
      "                  'rel': '',\n",
      "                  'tag': 'DT',\n",
      "                  'word': 'a'},\n",
      "             24: {'address': 24,\n",
      "                  'ctag': 'JJ',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '',\n",
      "                  'head': 25,\n",
      "                  'lemma': 'line-item',\n",
      "                  'rel': '',\n",
      "                  'tag': 'JJ',\n",
      "                  'word': 'line-item'},\n",
      "             25: {'address': 25,\n",
      "                  'ctag': 'NN',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [23, 24]}),\n",
      "                  'feats': '',\n",
      "                  'head': 22,\n",
      "                  'lemma': 'veto',\n",
      "                  'rel': '',\n",
      "                  'tag': 'NN',\n",
      "                  'word': 'veto'},\n",
      "             26: {'address': 26,\n",
      "                  'ctag': 'TO',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '',\n",
      "                  'head': 27,\n",
      "                  'lemma': 'to',\n",
      "                  'rel': '',\n",
      "                  'tag': 'TO',\n",
      "                  'word': 'to'},\n",
      "             27: {'address': 27,\n",
      "                  'ctag': 'VB',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [26, 30]}),\n",
      "                  'feats': '',\n",
      "                  'head': 21,\n",
      "                  'lemma': 'provoke',\n",
      "                  'rel': '',\n",
      "                  'tag': 'VB',\n",
      "                  'word': 'provoke'},\n",
      "             28: {'address': 28,\n",
      "                  'ctag': 'DT',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '',\n",
      "                  'head': 30,\n",
      "                  'lemma': 'a',\n",
      "                  'rel': '',\n",
      "                  'tag': 'DT',\n",
      "                  'word': 'a'},\n",
      "             29: {'address': 29,\n",
      "                  'ctag': 'NN',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '',\n",
      "                  'head': 30,\n",
      "                  'lemma': 'test',\n",
      "                  'rel': '',\n",
      "                  'tag': 'NN',\n",
      "                  'word': 'test'},\n",
      "             30: {'address': 30,\n",
      "                  'ctag': 'NN',\n",
      "                  'deps': defaultdict(<class 'list'>, {'': [28, 29]}),\n",
      "                  'feats': '',\n",
      "                  'head': 27,\n",
      "                  'lemma': 'case',\n",
      "                  'rel': '',\n",
      "                  'tag': 'NN',\n",
      "                  'word': 'case'},\n",
      "             31: {'address': 31,\n",
      "                  'ctag': '.',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '',\n",
      "                  'head': 5,\n",
      "                  'lemma': '.',\n",
      "                  'rel': '',\n",
      "                  'tag': '.',\n",
      "                  'word': '.'}})\n"
     ]
    }
   ],
   "source": [
    "# parsing takes a list of dependency graphs and a model as arguments\n",
    "parses = tp.parse(dependency_treebank.parsed_sents()[-10:], 'tp.model')\n",
    "print(len(parses))\n",
    "print(parses[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Evaluation of Dependency Parsing\n",
    "\n",
    "Dependency Parsing performance is evaluated as __labeled__ and __unlabeled attachment scores__ which are calculated as \n",
    "\n",
    "$$ UAS/LAS = \\frac{\\text{# of corrent dependency relations}}{\\text{# of dependency relations}}$$\n",
    "\n",
    "the difference between the two is whether the relation labels are considered or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "NLTK provides `DependencyEvaluator` class to perform the evaluation, that takes predicted and reference parses as arguments. The evaluation ignores punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7791666666666667\n",
      "0.7791666666666667\n"
     ]
    }
   ],
   "source": [
    "from nltk.parse import DependencyEvaluator\n",
    "\n",
    "de = DependencyEvaluator(parses, dependency_treebank.parsed_sents()[-10:])\n",
    "las, uas = de.eval()\n",
    "\n",
    "# no labels, thus identical\n",
    "print(las)\n",
    "print(uas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise\n",
    "- Train `arc-standard` and `arc-eager` transition parsers on the same portion (slightly bigger than 100, otherwise it takes a lot of time)\n",
    "- Evaluate both of them comparing the attachment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. Dependency Parsing with Stanza and Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Both spaCy and Stanza (python package Stanford NLP Tools) provide pre-trained dependency parsing models.\n",
    "The libraries are quite similar in usage.\n",
    "\n",
    "- initialize pipeline (with other processing steps such as tokenization, POS-tagging)\n",
    "- process a sentence \n",
    "- iterate over tokens accessing dependency parsing attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 5.1. Stanford Dependency Parser\n",
    "\n",
    "[paper on stanza](https://arxiv.org/pdf/2003.07082.pdf)\n",
    "\n",
    "A neural graph-based dependency parser. \n",
    "[paper on parser](https://nlp.stanford.edu/pubs/dozat2017deep.pdf)\n",
    "\n",
    "> We implement a Bi-LSTM-based deep biaffine neural dependency parser (Dozat and Manning, 2017). We\n",
    "further augment this model with two linguistically motivated features: one that predicts the linearization order of two words in a given language, and the other that predicts the typical distance in linear order between them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 5.2. Spacy Dependency Parser\n",
    "\n",
    "> A transition-based dependency parser component. The dependency parser jointly learns sentence segmentation and labelled dependency parsing, and can optionally learn to merge tokens that had been over-segmented by the tokenizer. The parser uses a variant of the non-monotonic arc-eager transition-system described by Honnibal and Johnson (2014), with the addition of a \"break\" transition to perform the sentence segmentation. Nivre (2005)’s pseudo-projective dependency transformation is used to allow the parser to predict non-projective parses.\n",
    "\n",
    "> The parser is trained using an imitation learning objective. It follows the actions predicted by the current weights, and at each state, determines which actions are compatible with the optimal parse that could be reached from the current state. The weights are updated such that the scores assigned to the set of optimal actions is increased, while scores assigned to other actions are decreased. Note that more than one action may be optimal for a given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "example = 'I saw the man with a telescope.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 10:07:45 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2022-04-18 10:07:45 INFO: Use device: cpu\n",
      "2022-04-18 10:07:45 INFO: Loading: tokenize\n",
      "2022-04-18 10:07:45 INFO: Loading: pos\n",
      "2022-04-18 10:07:45 INFO: Loading: lemma\n",
      "2022-04-18 10:07:45 INFO: Loading: depparse\n",
      "2022-04-18 10:07:45 INFO: Loading: sentiment\n",
      "2022-04-18 10:07:46 INFO: Loading: constituency\n",
      "2022-04-18 10:07:46 INFO: Loading: ner\n",
      "2022-04-18 10:07:47 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tI\t2\tnsubj\n",
      "2\tsaw\t0\troot\n",
      "3\tthe\t4\tdet\n",
      "4\tman\t2\tobj\n",
      "5\twith\t7\tcase\n",
      "6\ta\t7\tdet\n",
      "7\ttelescope\t2\tobl\n",
      "8\t.\t2\tpunct\n"
     ]
    }
   ],
   "source": [
    "# stanza example\n",
    "import stanza\n",
    "\n",
    "stanza_nlp = stanza.Pipeline(lang='en')\n",
    "stanza_doc = stanza_nlp(example)\n",
    "\n",
    "for sent in stanza_doc.sentences:\n",
    "    for word in sent.words:\n",
    "        print(\"{}\\t{}\\t{}\\t{}\".format(word.id, word.text, word.head, word.deprel))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tI\tsaw\tnsubj\n",
      "1\tsaw\tsaw\tROOT\n",
      "2\tthe\tman\tdet\n",
      "3\tman\tsaw\tdobj\n",
      "4\twith\tman\tprep\n",
      "5\ta\ttelescope\tdet\n",
      "6\ttelescope\twith\tpobj\n",
      "7\t.\tsaw\tpunct\n"
     ]
    }
   ],
   "source": [
    "# spacy example\n",
    "import spacy\n",
    "\n",
    "# spacy_nlp = spacy.load(\"en-core-web-sm\")\n",
    "\n",
    "# un-comment the lines below, if you get 'ModuleNotFoundError'\n",
    "import en_core_web_sm\n",
    "spacy_nlp = en_core_web_sm.load()\n",
    "\n",
    "spacy_doc = spacy_nlp(example)\n",
    "\n",
    "for sent in spacy_doc.sents:\n",
    "    for token in sent:\n",
    "        print(\"{}\\t{}\\t{}\\t{}\".format(token.i, token.text, token.head, token.dep_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Lab Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write functions to convert `spacy` and `stanza` dependency parses into `NLTK`'s [`DependencyGraph`](https://www.nltk.org/_modules/nltk/parse/dependencygraph.html)\n",
    "    - make use of `load` and [Malt-Tab format](https://cl.lingfil.uu.se/~nivre/research/MaltXML.html)\n",
    "- Parse 100 last sentences from dependency treebank using `spacy` and `stanza`\n",
    "- Evaluate the parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
