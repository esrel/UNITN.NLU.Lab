{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Statistical Language Modeling with NLTK\n",
    "\n",
    "- Evgeny A. Stepanov\n",
    "- stepanov.evgeny.a@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Objectives\n",
    "\n",
    "- Understanding:\n",
    "    - relation between lexicon and language model\n",
    "    - smoothing techniques \n",
    "    - OOV effects and remedies\n",
    "\n",
    "- Learning how to\n",
    "    - prepare data for ngram modeling\n",
    "    - count ngrams in a corpus\n",
    "    - train an ngram model with `NLTK`\n",
    "    - use ngram model to\n",
    "        - compute probability of a sequence\n",
    "        - generate a sequence\n",
    "    \n",
    "    - evaluate ngram model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recommended Reading\n",
    "\n",
    "- Dan Jurafsky and James H. Martin's __Speech and Language Processing__ ([3rd ed. draft](https://web.stanford.edu/~jurafsky/slp3/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Covered Material\n",
    "- SLP\n",
    "    - [Chapter 3: N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Requirements\n",
    "\n",
    "- [NLTK](https://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Ngrams and Ngram Counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[n-gram](https://en.wikipedia.org/wiki/N-gram) is a contiguous sequence of *n* items from a given sequence of text or speech. An n-gram model models sequences, notably natural languages, using the statistical properties of n-grams.\n",
    "\n",
    "__Example__:\n",
    "\n",
    "- character n-grams: cat\n",
    "- word n-grams: the cat is fat\n",
    "\n",
    "\n",
    "|                     | 1-gram  | 2-gram  | 3-gram  |\n",
    "|---------------------|---------|---------|---------|\n",
    "|                     | unigram | bigram  | trigram |\n",
    "| *Markov Order*      | 0       | 1       | 2       |\n",
    "| *Character N-grams* | `['c', 'a', 't']` | `['ca', 'at']` | `['cat']` |\n",
    "| *Word N-grams*      | `['the', 'cat', 'is' , 'fat']` | `['the cat', 'cat is', ...]` | `['the cat is', ...]` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1. Counting Ngrams\n",
    "\n",
    "*Frequency List* of a corpus is essentially a unigram count. Ngram count only differs in a unit of counting -- sequence of $n$ of tokens. We can compute bigram count by taking sequences of 2 items, trigrams - 3, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.1.1. Preparing Data\n",
    "The required data format for ngram counting is a _list-of-lists_ (lists of sentences consisting of lists of words): \n",
    "\n",
    "```\n",
    "[\n",
    "     ['the', 'cat', 'is', 'fat'], \n",
    "     ['the', 'dog', 'is', 'hungry']\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.1.2. Sentence beginning & end tags\n",
    "\n",
    "Including sentence boundary markers leads to a better model. To do that we need to augment each sentence with a special symbols for beginning and end of sentence tags (`<s>` and `</s>`, respectively). The beginning of the sentence tag gives the bigram context of the first word; and encodes probability of a word to start a sentence. Adding the end of the sentence tag, on the other hand, makes the bigram model a true probability distribution (Jurafsky and Martin). \"Without it, the sentence probabilities for all sentences of a given length would sum to one. This model would define an infinite set of probability distributions, with one distribution per sentence length.\"\n",
    "\n",
    "For larger ngrams, weâ€™ll need to assume extra context for the contexts to the left and right of the sentence boundaries. For example, to compute trigram probabilities at the very beginning of the sentence, we can use two pseudo-words for the first trigram (i.e. `['<s>', '<s>', w1]`). Alternatively, we can use [back-off](https://en.wikipedia.org/wiki/Katz%27s_back-off_model), and use the `['<s>', w1]` bigram probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Example**:\n",
    "`['<s>', 'the', 'cat', 'is', 'fat', '</s>']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.1.3. NLTK Utility Functions\n",
    "NLTK provides utility functions for padding sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sent = ['the', 'cat', 'is', 'fat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'the', 'cat', 'is', 'fat', '</s>']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import pad_sequence\n",
    "\n",
    "list(pad_sequence(\n",
    "        sent,  # input sequence\n",
    "        pad_left=True,\n",
    "        left_pad_symbol=\"<s>\",\n",
    "        pad_right=True,\n",
    "        right_pad_symbol=\"</s>\",\n",
    "        n=2  # padding for bigrams\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Another NLTK function wraps this utility function with default arguments to provide a more convenient interface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'the', 'cat', 'is', 'fat', '</s>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "\n",
    "list(pad_both_ends(sent, n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.1.4. Extracting Ngrams\n",
    "NLTK provides a function to extact ngrams from a sequence, which also performs padding, if required arguments are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 'cat'), ('cat', 'is'), ('is', 'fat')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "list(ngrams(sent, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', 'the'), ('the', 'cat'), ('cat', 'is'), ('is', 'fat'), ('fat', '</s>')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = ngrams(\n",
    "    sent,\n",
    "    2,\n",
    "    pad_left=True,\n",
    "    left_pad_symbol=\"<s>\",\n",
    "    pad_right=True,\n",
    "    right_pad_symbol=\"</s>\",\n",
    ")\n",
    "list(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Additionally, NLTK provides wrapper functions to extract bigrams and trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'cat'), ('cat', 'is'), ('is', 'fat')]\n",
      "[('the', 'cat', 'is'), ('cat', 'is', 'fat')]\n",
      "[('<s>', 'the'), ('the', 'cat'), ('cat', 'is'), ('is', 'fat'), ('fat', '</s>')]\n",
      "[('<s>', 'the'), ('the', 'cat'), ('cat', 'is'), ('is', 'fat'), ('fat', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import bigrams, trigrams\n",
    "\n",
    "print(list(bigrams(sent)))\n",
    "print(list(trigrams(sent)))\n",
    "\n",
    "print(list(bigrams(pad_both_ends(sent, n=2))))\n",
    "print(list(bigrams(sent, pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"</s>\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.1.5. Extracting \"Everygrams\"\n",
    "To make an ngram model more robust, it is usually also trained on lower-ngrams (i.e. unigrams in case of bigrams). \n",
    "NLTK also provides utility function to extract these together.\n",
    "\n",
    "Note the `max_len` argument that defines the maximum length of an ngram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the',), ('the', 'cat'), ('cat',), ('cat', 'is'), ('is',), ('is', 'fat'), ('fat',)]\n",
      "[('<s>',), ('<s>', 'the'), ('the',), ('the', 'cat'), ('cat',), ('cat', 'is'), ('is',), ('is', 'fat'), ('fat',), ('fat', '</s>'), ('</s>',)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import everygrams\n",
    "\n",
    "print(list(everygrams(sent, max_len=2)))\n",
    "print(list(everygrams(pad_both_ends(sent, n=2), max_len=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>',),\n",
       " ('<s>', 'the'),\n",
       " ('the',),\n",
       " ('the', 'cat'),\n",
       " ('cat',),\n",
       " ('cat', 'is'),\n",
       " ('is',),\n",
       " ('is', 'fat'),\n",
       " ('fat',),\n",
       " ('fat', '</s>'),\n",
       " ('</s>',)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ngrams = everygrams(\n",
    "    sent,\n",
    "    min_len=1,\n",
    "    max_len=2,\n",
    "    pad_left=True,\n",
    "    left_pad_symbol=\"<s>\",\n",
    "    pad_right=True,\n",
    "    right_pad_symbol=\"</s>\"\n",
    ")\n",
    "list(all_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.1.6. \"Flattening\" the Data\n",
    "For language model training and evaluation (as well as vocabulary extraction) NLTK expects the data to be a flat list. \n",
    "In python, this is accomplished either by using `itertools.chain.from_iterable` or python list comprehension as\n",
    "\n",
    "`[element for sublist in superlist for element in sublist]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'cat', 'is', 'fat', 'the', 'dog', 'is', 'hungry']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "data = [['the', 'cat', 'is', 'fat'], ['the', 'dog', 'is', 'hungry']]\n",
    "\n",
    "list(chain.from_iterable(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'cat', 'is', 'fat', 'the', 'dog', 'is', 'hungry']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token for sent in data for token in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'cat', 'is', 'fat', 'the', 'dog', 'is', 'hungry']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import flatten\n",
    "list(flatten(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.1.7. Combining the Tasks\n",
    "NLTK wraps both tasks:\n",
    "\n",
    "- padded ngram (everygram) extraction from each sentence\n",
    "- flat list creation\n",
    "\n",
    "into a convenient utility function `padded_everygram_pipeline` that returns lazy iterators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "padded_ngrams, flat_text = padded_everygram_pipeline(2, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>',), ('<s>', 'the'), ('the',), ('the', 'cat'), ('cat',), ('cat', 'is'), ('is',), ('is', 'fat'), ('fat',), ('fat', '</s>'), ('</s>',)]\n",
      "[('<s>',), ('<s>', 'the'), ('the',), ('the', 'dog'), ('dog',), ('dog', 'is'), ('is',), ('is', 'hungry'), ('hungry',), ('hungry', '</s>'), ('</s>',)]\n"
     ]
    }
   ],
   "source": [
    "for sent_ngrams in padded_ngrams:\n",
    "    print(list(sent_ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'the',\n",
       " 'cat',\n",
       " 'is',\n",
       " 'fat',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'the',\n",
       " 'dog',\n",
       " 'is',\n",
       " 'hungry',\n",
       " '</s>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(flat_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.2. Ngram Counter\n",
    "NLTK provides NgramCounter class to do the ngram counting. The class can be initialized without any agrument and then updated with ngrams (via `update()` method); or directly initialized with ngrams.\n",
    "\n",
    "`N()` method returns total number of ngrams stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.lm import NgramCounter\n",
    "\n",
    "counter = NgramCounter()\n",
    "counter.N()  # return total number of stored ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_ngrams, flat_text = padded_everygram_pipeline(2, data)\n",
    "counter.update(padded_ngrams)  # update counter with ngrams\n",
    "counter.N()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_ngrams, flat_text = padded_everygram_pipeline(2, data)\n",
    "counter = NgramCounter(padded_ngrams)\n",
    "counter.N()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.2.1. Accessing Ngram Counts\n",
    "\n",
    "Ngram counts can be accessed using standard python dictionary notation. \n",
    "- Ngram order counts can be accessed using integer keys (order)\n",
    "    - returns `Frequency Distribution` or `Conditional Frequency Distribution` objects. [link](https://www.nltk.org/api/nltk.probability.html)\n",
    "- Unigram counts can be accessed using string keys.\n",
    "- Bigram counts can be accessed using list keys & string keys (see example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 8 samples and 12 outcomes>\n",
      "<ConditionalFreqDist with 7 conditions>\n"
     ]
    }
   ],
   "source": [
    "# ngram order counts\n",
    "print(counter[1])  # Frequency Distribution\n",
    "print(counter[2])  # Conditional Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 8 samples and 12 outcomes>\n"
     ]
    }
   ],
   "source": [
    "# print counts\n",
    "print(counter.unigrams)  # unigram count for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unigram counts\n",
    "counter['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'full' bigram counts (full bigrams)\n",
    "counter[['the']]['cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'cat': 1, 'dog': 1})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get a frequency distribution over all continuations, you can use list or tuple. \n",
    "# counter[['the']] or counter[('the',)]\n",
    "counter[['the']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'cat': 1, 'dog': 1})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Conditional Frequency Distributions, only tuples are accepted.\n",
    "# counter[2][['the']] with throw an error\n",
    "counter[2][('the',)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise: Counting Ngrams in Shakespeare's Hamlet\n",
    "\n",
    "- Load Shakespeare's Hamlet from Gutenberg corpus\n",
    "    - lowercase it\n",
    "\n",
    "- Extract padded unigrams and bigrams\n",
    "\n",
    "- Using NgramCounter\n",
    "    - get total number of ngrams\n",
    "    - get count of unigram `the`\n",
    "    - get count of bigram `of the`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3106\n",
      "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', 'William', 'Shakespeare', '1599', ']']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "hamlet = gutenberg.sents('shakespeare-hamlet.txt')\n",
    "\n",
    "print(len(hamlet))\n",
    "print(hamlet[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'the', 'tragedie', 'of', 'hamlet', 'by', 'william', 'shakespeare', '1599', ']']\n"
     ]
    }
   ],
   "source": [
    "# lowercasing\n",
    "hamlet = [[w.lower() for w in sent] for sent in hamlet]\n",
    "print(hamlet[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Vocabulary and Basic Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[`Vocabulary`](https://www.nltk.org/api/nltk.lm.vocabulary.html) class of NLTK satisfies two common language modeling requirements for a vocabulary:\n",
    "- When checking membership and calculating its size, filters items by comparing their counts to a cutoff value.\n",
    "- Adds a special \"unknown\" token which unseen words are mapped to.\n",
    "\n",
    "Tokens with counts greater than or equal to the cut-off value will be considered part of the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`Vocabulary(counts=None, unk_cutoff=1, unk_label='<UNK>)` create a new `Vocabulary`.\n",
    "\n",
    "- `counts` (optional) iterable or collections. \n",
    "    - Counter instance to pre-seed the `Vocabulary`. \n",
    "    - In case it is iterable, counts are calculated.\n",
    "\n",
    "- `unk_cutoff` (int) - Words that occur less frequently than this value are not considered part of the vocabulary.\n",
    "\n",
    "- `unk_label` - Label for marking words not part of vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.lm import Vocabulary\n",
    "hamlet_words = gutenberg.words('shakespeare-hamlet.txt')\n",
    "\n",
    "# lowercase\n",
    "hamlet_words = [w.lower() for w in hamlet_words]\n",
    "\n",
    "# initialize vocabulary with cut-off\n",
    "vocab = Vocabulary(hamlet_words, unk_cutoff=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`Vocabulary` methods:\n",
    "\n",
    "- `lookup()` looks up words in a vocabulary. \n",
    "    - \"Unseen\" words (with counts less than cutoff) are looked up as the unknown label. \n",
    "    - If given one word (a string) as an input, this method will return a string.\n",
    "    - If given a sequence, it will return an tuple of the looked up words.\n",
    "    \n",
    "- `update()` update counts from a sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`Vocabulary` properties:\n",
    "\n",
    "- `cutoff` - same as `unk_cutoff`\n",
    "\n",
    "- `unk_label` and `counts` can also be accessed as properties\n",
    "    - `vocab.unk_label`\n",
    "    - `vocab.counts`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.1. Counts, Vocabulary Membership, and Lookup\n",
    "\n",
    "Tokens with frequency counts less than the `cutoff` value will be considered not part of the vocabular, even though their entries in the count dictionary are preserved. \n",
    "\n",
    "This is useful for changing cut-off without recomputing counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# let's define a function to illustrate the relation between counts, membership, and lookup\n",
    "def test_word(word, vocab):\n",
    "    # let's lowercase it\n",
    "    if word != vocab.unk_label:\n",
    "        word = word.lower() \n",
    "    # membership test\n",
    "    word_membership = word in vocab\n",
    "    # accessing count\n",
    "    word_count = vocab[word]\n",
    "    # lookup\n",
    "    word_lookup = vocab.lookup(word)\n",
    "    \n",
    "    print(\"Word: '{}', Count: {}, In Vocab: {}, Mapped to: '{}'\".format(\n",
    "        word, word_count, word_membership, word_lookup))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: '<UNK>', Count: 2, In Vocab: True, Mapped to: '<UNK>'\n",
      "Word: 'the', Count: 993, In Vocab: True, Mapped to: 'the'\n",
      "Word: '1599', Count: 1, In Vocab: False, Mapped to: '<UNK>'\n",
      "Word: 'trento', Count: 0, In Vocab: False, Mapped to: '<UNK>'\n"
     ]
    }
   ],
   "source": [
    "for w in [\"<UNK>\", \"the\", \"1599\", \"Trento\"]:\n",
    "    test_word(w, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.2. Cut-Off and Vocabulary Size\n",
    "The cut-off value influences not only membership checking but also the result of getting the size of the vocabulary using the built-in `len`. Note that while the number of keys in the vocabulary's counter stays the same, the items in the vocabulary differ depending on the cut-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Let's define another vocabulary with cut-off 1\n",
    "# Notice that we are passing counts from the original vocabulary\n",
    "vocab1 = Vocabulary(vocab.counts, unk_cutoff=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CutOff 2: 1867\n",
      "CutOff 1: 4717\n"
     ]
    }
   ],
   "source": [
    "print(\"CutOff 2:\", len(vocab))\n",
    "print(\"CutOff 1:\", len(vocab1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CutOff 2 Counts: 4716\n",
      "CutOff 1 Counts: 4716\n"
     ]
    }
   ],
   "source": [
    "print(\"CutOff 2 Counts:\", len(vocab.counts))\n",
    "print(\"CutOff 1 Counts:\", len(vocab1.counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise\n",
    "- lookup in vocabulary\n",
    "    - \"trento\"\n",
    "    - \"trento is the capital city of trentino\"\n",
    "        - split into a list\n",
    "- update vocabulary with \"trento is the capital city of trentino\" (splitting)\n",
    "    - do the lookup again to see the effect\n",
    "- create another vocabulary changing the cut-off to `1`\n",
    "    - do the lookup again to see the effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Training Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Having prepared the data as described above, the ngram model training with `NLTK` boils down to counting ngrams as presented above, specifying the highest order ngram size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's prepare data on hamlet\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "data = [[w.lower() for w in sent] for sent in gutenberg.sents('shakespeare-hamlet.txt')]\n",
    "padded_ngrams, flat_text = padded_everygram_pipeline(2, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's train a `Maximum Likelihood Estimator (MLE)\n",
    "from nltk.lm import MLE\n",
    "\n",
    "mle_lm = MLE(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Initialization of the Language Model creates:\n",
    "- empty vocabulary\n",
    "- empty counts\n",
    "\n",
    "These are populated once we `fit` the model with training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 0 items>\n",
      "<NgramCounter with 1 ngram orders and 0 ngrams>\n"
     ]
    }
   ],
   "source": [
    "print(mle_lm.vocab)\n",
    "print(mle_lm.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mle_lm.fit(padded_ngrams, flat_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 4719 items>\n",
      "<NgramCounter with 2 ngram orders and 84038 ngrams>\n"
     ]
    }
   ],
   "source": [
    "print(mle_lm.vocab)\n",
    "print(mle_lm.counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Using Ngram Language Models\n",
    "\n",
    "A statistical [language model](https://en.wikipedia.org/wiki/Language_model) is a probability distribution over sequences of words. Given such a sequence, say of length $n$, it assigns a probability $P(w_{1},\\ldots ,w_{n})$ ($P(w_{1}^{n})$, for compactness) to the whole sequence (using Chain Rule). Consequently, the unigram and bigram probabilities computed above constitute an ngram language model of our corpus.\n",
    "\n",
    "It is more useful for Natural Language Processing to have a __probability__ of a sequence being legal, rather than a grammar's __boolean__ decision whether it is legal or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.1. Computing Probability of a Sequence (Scoring)\n",
    "\n",
    "The most common usage of a language model is to compute probability of a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Probability of a Sequence\n",
    "\n",
    "Probability of a sequence is computed as a product of conditional probabilities ([Chain Rule](https://en.wikipedia.org/wiki/Chain_rule_(probability))). \n",
    "\n",
    "$$P(w_{1}^{n}) = P(w_1) P(w_2|w_1) P(w_3|w_1^2) ... P(w_n|w_{1}^{n-1}) = \\prod_{i=1}^{n}{P(w_i|w_{1}^{i-1})}$$\n",
    "\n",
    "The order of ngram makes a simplifying assumption that probability of a current word only depends on previous $N - 1$ elements. Thus, it truncates previous context (history) to length $N - 1$.\n",
    "\n",
    "$$P(w_i|w_{1}^{i-1}) \\approx P(w_i|w_{i-N+1}^{i-1})$$\n",
    "\n",
    "Consequently we have:\n",
    "\n",
    "N-gram   | Equation                   |\n",
    ":--------|:---------------------------|\n",
    "unigram  | $$P(w_i)$$                 |\n",
    "bigram   | $$P(w_i|w_{i-1})$$         |\n",
    "trigram  | $$P(w_i|w_{i-2},w_{i-1})$$ |\n",
    "\n",
    "The probability of the whole sequence applying an ngram model becomes:\n",
    "\n",
    "$$P(w_{1}^{n}) = \\prod_{i=1}^{n}{P(w_i|w_{i-N+1}^{i-1})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Calculating Probability from Frequencies\n",
    "\n",
    "Probabilities of ngrams can be computed *normalizing* frequency counts (*Maximum Likelihood Estimation*): dividing the frequency of an ngram sequence by the frequency of its prefix (*relative frequency*).\n",
    "\n",
    "N-gram   | Equation                      \n",
    ":--------|:------------------------------\n",
    "Unigram  | $$p(w_i) = \\frac{c(w_i)}{N}$$ \n",
    "Bigram   | $$p(w_i | w_{i-1}) = \\frac{c(w_{i-1}, w_i)}{c(w_{i-1})}$$ \n",
    "Ngram    | $$p(w_i | w_{i-n+1}^{i-1}) = \\frac{c(w_{i-n+1}^{i-1}, w_i)}{c(w_{i-n+1}^{i-1})}$$ \n",
    "\n",
    "where:\n",
    "- $N$ is the total number of words in a corpus\n",
    "- $c(x)$ is the count of occurrences of $x$ in a corpus (x could be unigram, bigram, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4.1.1. Scoring Methods of NLTK LMs\n",
    "- `score(word, context=None)` Masks out of vocab (OOV) words (i.e. maps them to `<UNK>`) and computes their model score.\n",
    "    - scores are **model-specific** (later on this)\n",
    "    - Parameters:\n",
    "        - `word (str)` - Word for which we want the score\n",
    "        - `context (tuple(str))` - Context the word is in. If None, computes unigram score.\n",
    "        \n",
    "- `logscore(word, context=None)` - Evaluate the log score of this word in this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02278986505095015"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A little illustration of the methods\n",
    "mle_lm.score(\"the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09672131147540984"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle_lm.score(\"the\", [\"of\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.3700223830884077"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle_lm.logscore(\"the\", [\"of\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise\n",
    "Implement a function to compute score of a sequence (i.e. Chain Rule)\n",
    "\n",
    "- arguments:\n",
    "    - Language Model\n",
    "    - List of Tokens\n",
    "\n",
    "- functionality\n",
    "    - extracts ngrams w.r.t. LM order (`lm.order`)\n",
    "    - scores each ngram w.r.t. LM (`lm.score` or `lm.logscore`)\n",
    "        - mind that `score` takes care of OOV by conterting to `<UNK>` already\n",
    "    - computes the overal score using chain rule\n",
    "        - mind the difference between `score` and `logscore`\n",
    "\n",
    "- compute the scores of the sentences below\n",
    "    - compute padded and unpadded sequence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "test_sents = [\"the king is dead\", \"the tzar is dead\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4.1.2. OOV in MLE\n",
    "In MLE LM we did not take care of OOV. Consequently, those have `0` counts and probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(mle_lm.score(\"<UNK>\"))\n",
    "print(mle_lm.score(\"<UNK>\", [\"<UNK>\"]))\n",
    "print(mle_lm.score(\"<UNK>\", [\"the\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# same as above: getting lowercaseed sentences and words \n",
    "hamlet_sents = [[w.lower() for w in sent] for sent in gutenberg.sents('shakespeare-hamlet.txt')]\n",
    "hamlet_words = flatten(hamlet_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1867\n"
     ]
    }
   ],
   "source": [
    "# computing vocabulary with cutoff\n",
    "lex = Vocabulary(hamlet_words, unk_cutoff=2)\n",
    "print(len(lex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'the', 'tragedie', 'of', 'hamlet', 'by', '<UNK>', '<UNK>', '<UNK>', ']']\n"
     ]
    }
   ],
   "source": [
    "# replacing words with counts below cutoff with '<UNK>'\n",
    "hamlet_oov_sents = [list(lex.lookup(sent)) for sent in hamlet_sents]\n",
    "print(hamlet_oov_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# extracting ngrams & words again\n",
    "padded_ngrams_oov, flat_text_oov = padded_everygram_pipeline(2, hamlet_oov_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lm_oov = MLE(2)\n",
    "lm_oov.fit(padded_ngrams_oov, flat_text_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06540897824290828\n",
      "0.06842105263157895\n",
      "0.24874118831822759\n"
     ]
    }
   ],
   "source": [
    "print(lm_oov.score(\"<UNK>\"))\n",
    "print(lm_oov.score(\"<UNK>\", [\"<UNK>\"]))\n",
    "print(lm_oov.score(\"<UNK>\", [\"the\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4.1.3. Data Sparsity: Missing Ngrams\n",
    "However, even though we have counted ngrams on the data set with `<UNK>`, the counts still do not account for all possible ngrams. Thus, some possible sequences will have zero probability.\n",
    "\n",
    "__We will address this later.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00018360414945377767\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(lm_oov.score(\"queen\"))\n",
    "print(lm_oov.score(\"<UNK>\", [\"queen\"]))\n",
    "print(lm_oov.score(\"queen\", [\"<UNK>\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.2. Generating Sequences\n",
    "\n",
    "Ngram Model can be used as an automaton to generate probable legal sequences using the algorithm below.\n",
    "\n",
    "__Algorithm for Bigram LM__\n",
    "\n",
    "- $w_{i-1} = $ `<s>`;\n",
    "- *while* $w_i \\neq $ `</s>`\n",
    "\n",
    "    - stochastically get new word w.r.t. $P(w_i|w_{i-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4.2.1. Generation Method of NLTK LM\n",
    "- `generate(num_words=1, text_seed=None, random_seed=None)` generates words from the model.\n",
    "    - Parameters\n",
    "        - `num_words (int)` - How many words to generate. By default 1.\n",
    "        - `text_seed` - preceding context the generation should be conditioned on.\n",
    "            - ngram model is restricted in how much preceding context it can take into account based on max ngram size\n",
    "        - `random_seed` - A random seed. If provided, makes the random sampling part of generation reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>', 'of', 'kings', 'in', 'the']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle_lm.generate(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', '</s>', 'beene', 'most', 'like']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle_lm.generate(5, text_seed=['king'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.3. Language Model Evaluation\n",
    "\n",
    "Language Models are evaluated using [Perplexity](https://en.wikipedia.org/wiki/Perplexity)\n",
    "\n",
    "- Measures how well model fits test data\n",
    "- Probability of test data\n",
    "- Weighted average branching factor in predicting the next word (lower is better).\n",
    "- Computed as:\n",
    "\n",
    "$$ PP(W) = \\sqrt[N]{\\frac{1}{P(w_1,w_2,...,w_N)}} = \\sqrt[N]{\\frac{1}{\\prod_{i=1}^{N}P(w_i|w_{i-N+1})}}$$\n",
    "\n",
    "Where $N$ is the number of words in test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4.3.1. Evaluation Methods of NLTK LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`NLTK` provides both **perplexity** and **cross-entropy** as evaluation methods\n",
    "\n",
    "- `entropy(text_ngrams)` calculates cross-entropy of model for given evaluation text.\n",
    "    - Parameters\n",
    "        - `text_ngrams (Iterable(tuple(str)))` A sequence of ngram tuples.\n",
    "\n",
    "- `perplexity(text_ngrams)` calculates the perplexity of the given text.\n",
    "    - This is _2 ** cross-entropy_ for the text, so the arguments are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Perplexity** is related to [**Cross-Entropy**](https://en.wikipedia.org/wiki/Cross_entropy) as:\n",
    "\n",
    "$$PP(p) = 2^{H(p)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise\n",
    "Compute entropy and perplexity of the `MLE` models (with and without OOV handling) on the bigrams of the test sentences below, treating them as a test set.\n",
    "\n",
    "- extract bigrams (padded and unpadded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_sents = [\"the king is dead\", \"the tzar is dead\", \"the queen is dead\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.4. Handling Data Sparseness: Smoothing in NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[`Smoothing(vocabulary, counter, **kwargs)`](https://www.nltk.org/api/nltk.lm.smoothing.html) Class is initialized with the following parameters: \n",
    "\n",
    "- `vocabulary (nltk.lm.vocab.Vocabulary)` - The Ngram vocabulary object.\n",
    "- `counter (nltk.lm.counter.NgramCounter)` - The counts of the vocabulary items.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`Ngram Smoothing Interface` implements [Chen & Goodman 1995](https://aclanthology.org/P96-1041.pdf)'s idea that all smoothing algorithms have certain features in common. Consequently, each Smoothing subclass implements two methods:\n",
    "\n",
    "- `unigram_score(word)` return unigram score\n",
    "- `alpha_gamma(word, context)` returns alpha and gamma values (varies w.r.t. method)\n",
    "    - `gamma` is the value added to missing ngram count\n",
    "    - `alpha` is the value used to scale the lower order probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following smoothing methods are implemented:\n",
    "- `WittenBell(vocabulary, counter, **kwargs)` - Witten-Bell smoothing\n",
    "\n",
    "- `AbsoluteDiscounting(vocabulary, counter, discount=0.75, **kwargs)` - Smoothing with absolute discount\n",
    "    - takes `discount=0.75` parameter (default 0.75)\n",
    "    \n",
    "- `KneserNey(vocabulary, counter, order, discount=0.1, **kwargs)` - Kneser-Ney Smoothing (an extension of smoothing with a discount)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.5. NLTK Language Models\n",
    "\n",
    "All the language models in NLTK share the same interface and share methods for\n",
    "- evaluation\n",
    "- generation\n",
    "- scoring "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Scoring** in a language model does the following:\n",
    "- takes care of OOV words (see above)\n",
    "- computes `unmasked_score`\n",
    "\n",
    "The way `unmasked_score` is computed from counts is the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Some langauge models use **smoothing** to account for missing ngrams, some do not (e.g. `MLE`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Available LM Implementations\n",
    "\n",
    "- `MLE` - providing MLE ngram model scores.\n",
    "\n",
    "- Additive Smoothing LMs\n",
    "    - `Laplace` - Laplace (add one) smoothing ($\\gamma = 1$)\n",
    "    - `Lidstone` - same as `Laplace`, but adds $\\gamma$\n",
    "\n",
    "- Back-Off Language Models\n",
    "    - `StupidBackoff` - uses `alpha` to scale the lower order probabilities for computing missing ngram scores\n",
    "\n",
    "- Interpolated Language Models\n",
    "    - `WittenBellInterpolated` - Interpolated version of Witten-Bell smoothing\n",
    "    - `KneserNeyInterpolated` - Interpolated version of Kneser-Ney smoothing.\n",
    "    - `AbsoluteDiscountingInterpolated` - Interpolated version of smoothing with absolute discount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lab Exercise: Language Model Evaluation\n",
    "\n",
    "Evaluate different LMs trained on Shakespeare's Hamlet on Macbeth\n",
    "- Vary LM parameters\n",
    "    - ngram size `[2,3]`\n",
    "    - alpha, discount\n",
    "- Compute\n",
    "    - entropy\n",
    "    - preprlexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "macbeth_sents = [[w.lower() for w in sent] for sent in gutenberg.sents('shakespeare-macbeth.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
