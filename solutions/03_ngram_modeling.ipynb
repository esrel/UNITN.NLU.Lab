{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Solutions for Statistical Language Modeling with NLTK\n",
    "\n",
    "- Evgeny A. Stepanov\n",
    "- stepanov.evgeny.a@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise: Counting Ngrams in Shakespeare's Hamlet\n",
    "\n",
    "- Load Shakespeare's Hamlet from Gutenberg corpus\n",
    "    - lowercase it\n",
    "\n",
    "- Extract padded unigrams and bigrams\n",
    "\n",
    "- Using NgramCounter\n",
    "    - get total number of ngrams\n",
    "    - get count of unigram `the`\n",
    "    - get count of bigram `of the`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3106\n",
      "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', 'William', 'Shakespeare', '1599', ']']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "hamlet = gutenberg.sents('shakespeare-hamlet.txt')\n",
    "\n",
    "print(len(hamlet))\n",
    "print(hamlet[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'the', 'tragedie', 'of', 'hamlet', 'by', 'william', 'shakespeare', '1599', ']']\n"
     ]
    }
   ],
   "source": [
    "# lowercasing\n",
    "hamlet = [[w.lower() for w in sent] for sent in hamlet]\n",
    "print(hamlet[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import NgramCounter\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "padded_ngrams, flat_text = padded_everygram_pipeline(2, hamlet)\n",
    "counter = NgramCounter(padded_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84038"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of ngrams\n",
    "counter.N()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "993"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of unigram \"the\"\n",
    "counter[\"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of bigram \"of the\"\n",
    "counter[[\"of\"]][\"the\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise on Vocabulary\n",
    "- lookup in vocabulary\n",
    "    - \"trento\"\n",
    "    - \"trento is the capital city of trentino\"\n",
    "        - split into a list\n",
    "- update vocabulary with \"trento is the capital city of trentino\" (splitting)\n",
    "    - do the lookup again to see the effect\n",
    "- create another vocabulary changing the cut-off to `1`\n",
    "    - do the lookup again to see the effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.lm import Vocabulary\n",
    "hamlet_words = gutenberg.words('shakespeare-hamlet.txt')\n",
    "\n",
    "# lowercase\n",
    "hamlet_words = [w.lower() for w in hamlet_words]\n",
    "\n",
    "# initialize vocabulary with cut-off\n",
    "vocab = Vocabulary(hamlet_words, unk_cutoff=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<UNK>'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup(\"trento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<UNK>', 'is', 'the', '<UNK>', 'city', 'of', '<UNK>')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup(\"trento is the capital city of trentino\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<UNK>', 'is', 'the', '<UNK>', 'city', 'of', '<UNK>')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# updating vocabulary & doing lookup again\n",
    "# there is no effect because of cut-off\n",
    "vocab.update(\"trento is the capital city of trentino\".split())\n",
    "vocab.lookup(\"trento is the capital city of trentino\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('trento', 'is', 'the', 'capital', 'city', 'of', 'trentino')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another vocabulary with lower cut-off\n",
    "vocab1 = Vocabulary(vocab.counts, unk_cutoff=1)\n",
    "vocab1.lookup(\"trento is the capital city of trentino\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: Chain Rule\n",
    "Implement a function to compute score of a sequence (i.e. Chain Rule)\n",
    "\n",
    "- arguments:\n",
    "    - Language Model\n",
    "    - List of Tokens\n",
    "\n",
    "- functionality\n",
    "    - extracts ngrams w.r.t. LM order (`lm.order`)\n",
    "    - scores each ngram w.r.t. LM (`lm.score` or `lm.logscore`)\n",
    "        - mind that `score` takes care of OOV by converting to `<UNK>` already\n",
    "    - computes the overal score using chain rule\n",
    "        - mind the difference between `score` and `logscore`\n",
    "\n",
    "- compute the scores of the sentences below\n",
    "    - compute padded and unpadded sequence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "test_sents = [\"the king is dead\", \"the tzar is dead\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# use pad to make it compute both padded and unpadded scores\n",
    "def score_seq(lm, sequence, pad=True):\n",
    "    if pad:\n",
    "        sequence = pad_both_ends(sequence, n=lm.order)\n",
    "    seq_ngrams = list(ngrams(sequence, lm.order))\n",
    "    # simple for bigrams\n",
    "    # w.r.t. Jurafsky we do not score first unigram, since we have '<s>'\n",
    "    # and the first bigram will give us probability of a word to start a sentence\n",
    "    print(seq_ngrams)\n",
    "    seq_scores = [lm.logscore(ng[-1], ng[:-1]) for ng in seq_ngrams]\n",
    "    return sum(seq_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# let's prepare MLE LM\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "data = [[w.lower() for w in sent] for sent in gutenberg.sents('shakespeare-hamlet.txt')]\n",
    "padded_ngrams, flat_text = padded_everygram_pipeline(2, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "\n",
    "mle_lm = MLE(2)\n",
    "mle_lm.fit(padded_ngrams, flat_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>', 'the'), ('the', 'king'), ('king', 'is'), ('is', 'dead'), ('dead', '</s>')]\n",
      "-25.988633342954444\n",
      "[('<s>', 'the'), ('the', 'tzar'), ('tzar', 'is'), ('is', 'dead'), ('dead', '</s>')]\n",
      "-inf\n"
     ]
    }
   ],
   "source": [
    "# let's score padded sentences\n",
    "for sent in test_sents:\n",
    "    print(score_seq(mle_lm, sent.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
