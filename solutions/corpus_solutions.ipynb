{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Preprocessing using Python: Solutions\n",
    "\n",
    "- Natural Language Understanding\n",
    "- Evgeny A. Stepanov\n",
    "- stepanov.evgeny.a@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Requirements__\n",
    "\n",
    "- [NL2SparQL4NLU](https://github.com/esrel/NL2SparQL4NLU) dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(corpus_file):\n",
    "    \"\"\"\n",
    "    read corpus into a list-of-lists, splitting sentences into tokens by space (' ')\n",
    "    :param corpus_file: corpus file in sentence-per-line format (tokenized)\n",
    "    :return: corpus as list of lists\n",
    "    \"\"\"\n",
    "    return [line.strip().split() for line in open(corpus_file, 'r')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Sentence Beginning and End Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_add_tags(corpus, bos='<s>', eos='</s>'):\n",
    "    \"\"\"\n",
    "    add beginning-of-sentence (bos) and end-of-sentence (eos) tags\n",
    "    :param corpus: corpus as list-of-lists\n",
    "    :param bos: beginning-of-sentence tag\n",
    "    :param eos: end-of-sentence tag\n",
    "    \"\"\"\n",
    "    return [[bos] + sent + [eos] for sent in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Unknown Words\n",
    "Even though it is easier to handle unknown words by creating a custom lexicon file; this requires knowledge of the file formats (which we will cover later). Consequently, let's pre-process training data using python (and no libraries), from previous lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexicon Frequency Cut-Off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computing Frequency List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_frequency_list(corpus):\n",
    "    \"\"\"\n",
    "    create frequency list for a corpus\n",
    "    :param corpus: corpus in list-of-lists format\n",
    "    :return: frequency list as dict of counts\n",
    "    \"\"\"\n",
    "    frequencies = {}\n",
    "    for sentence in corpus:\n",
    "        for token in sentence:\n",
    "            frequencies[token] = frequencies.setdefault(token, 0) + 1 \n",
    "    return frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Applying min and max Frequency Cut-Off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutoff(frequency_list, tf_min=1, tf_max=float('inf')):\n",
    "    \"\"\"\n",
    "    apply min and max cutoffs to a frequency list\n",
    "    :param frequency_list: frequency list of a corpus as dict\n",
    "    :param tf_min: minimum token frequency for lexicon elements (below removed); default 1\n",
    "    :param tf_max: maximum token frequency for lexicon elements (above removed); default infinity\n",
    "    :return: lexicon as sorted list of tokens\n",
    "    \"\"\"\n",
    "    return sorted([token for token, frequency in frequency_list.items() if tf_max >= frequency >= tf_min])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Stop Words from Lexicon\n",
    "- Not used for Ngram Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(lexicon, stopwords):\n",
    "    \"\"\"\n",
    "    remove stopwords from a lexicon\n",
    "    :param lexicon: lexicon as a list\n",
    "    :param stopwords: stopwords list\n",
    "    :return: sorted difference of two lists (lexicon - stopwords)\n",
    "    \"\"\"\n",
    "    return sorted(list(set(lexicon) - set(stopwords)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing Lexicon from Corpus and Reading Lexicon from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lexicon(lexicon_file):\n",
    "    \"\"\"\n",
    "    read lexicon into a list\n",
    "    :param lexicon_file: lexicon file in token-per-line format\n",
    "    :return: lexicon as a list\n",
    "    \"\"\"\n",
    "    return [line.strip() for line in open(lexicon_file, 'r')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lexicon(corpus):\n",
    "    \"\"\"\n",
    "    compute lexicon of a corpus\n",
    "    :param corpus: corpus as list-of-lists\n",
    "    :return: sorted list of unique words\n",
    "    \"\"\"\n",
    "    return sorted(list(set([word for sent in corpus for word in sent])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing Unknown Words (OOV) with `<unk>` in a Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_replace_oov(corpus, lexicon, unk='<unk>'):\n",
    "    \"\"\"\n",
    "    replace all tokens that are not in lexicon with unk\n",
    "    :param corpus: corpus as list-of-lists\n",
    "    :param lexicon: lexicon as a list of tokens\n",
    "    :return: processed corpus\n",
    "    \"\"\"\n",
    "    return [[token if token in lexicon else unk for token in sent] for sent in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Training and Test Sets\n",
    "\n",
    "- Lexicon is computed only using training data\n",
    "- Both training and test sets are augmented with BOS (`<s>`) and EOS (`</s>`) tags\n",
    "- Both training and test sets have OOV words replaced with `<unk>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn='NL2SparQL4NLU/dataset/NL2SparQL4NLU.train.utterances.txt'\n",
    "tst='NL2SparQL4NLU/dataset/NL2SparQL4NLU.test.utterances.txt'\n",
    "trn_out='NL2SparQL4NLU.trn.data'\n",
    "tst_out='NL2SparQL4NLU.tst.data'\n",
    "\n",
    "trn_raw = read_corpus(trn)\n",
    "trn_tag = corpus_add_tags(trn_raw)\n",
    "trn_lex = cutoff(compute_frequency_list(trn_tag), tf_min=2)\n",
    "trn_unk = corpus_replace_oov(trn_tag, trn_lex)\n",
    "\n",
    "# write training data to a file\n",
    "with open(trn_out, 'w') as f:\n",
    "    for sent in trn_unk:\n",
    "        f.write(\" \".join(sent) + \"\\n\")\n",
    "        \n",
    "tst_raw = read_corpus(tst)\n",
    "tst_tag = corpus_add_tags(tst_raw)\n",
    "tst_unk = corpus_replace_oov(tst_tag, trn_lex)\n",
    "\n",
    "# write test data to a file\n",
    "with open(tst_out, 'w') as f:\n",
    "    for sent in tst_unk:\n",
    "        f.write(\" \".join(sent) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Corpus Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence and Word Counts\n",
    "- sentence count\n",
    "- word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_stats(corpus):\n",
    "    \"\"\"\n",
    "    compute word and sentence counts of the corpus\n",
    "    :param corpus: corpus as list-of-lists\n",
    "    :return: sentence count, word count\n",
    "    \"\"\"\n",
    "    return len(corpus), sum([len(sent) for sent in corpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicon Size, etc.\n",
    "- Length of lexicon list\n",
    "- Lexicon overlap (e.g. with stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_overlap(a, b):\n",
    "    \"\"\"\n",
    "    compute overal of two lists as set intersection\n",
    "    :param a: list 1\n",
    "    :param b: list 2\n",
    "    :return: sorted list of overlapping elements\n",
    "    \"\"\"\n",
    "    return sorted(list(set(a) & set(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "953\n",
      "571\n",
      "144\n",
      "(3338, 28129)\n"
     ]
    }
   ],
   "source": [
    "swl = 'NL2SparQL4NLU/extras/english.stop.txt'\n",
    "stopwords = read_lexicon(swl)\n",
    "trn_lex = compute_lexicon(trn_unk)\n",
    "\n",
    "print(len(trn_lex))\n",
    "print(len(stopwords))\n",
    "print(len(compute_overlap(trn_lex, stopwords)))\n",
    "\n",
    "print(corpus_stats(trn_unk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
