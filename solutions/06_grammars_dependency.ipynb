{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Solutions for Dependency Grammars with NLTK\n",
    "\n",
    "- Evgeny A. Stepanov\n",
    "- stapanov.evgeny.a@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise #1\n",
    "\n",
    "- Define grammar that covers the following sentences.\n",
    "\n",
    "    - show flights from new york to los angeles\n",
    "    - list flights from new york to los angeles\n",
    "    - show flights from new york\n",
    "    - list flights to los angeles\n",
    "    - list flights\n",
    "    \n",
    "- Use one of the parsers to parse the sentences (i.e. test your grammar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "The grammar below covers the sentences above & allows ambiguous parses.\n",
    "\n",
    "Ambiguity is caused by `'from' -> 'angeles' | 'york'` and `'to' -> 'angeles' | 'york'` in order to allow both `from new york` and `from los angeles`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "atis_rules = \"\"\"\n",
    "    'show' -> 'flights'\n",
    "    'list' -> 'flights'\n",
    "    'flights' -> 'from' | 'to'\n",
    "    'to' -> 'angeles' | 'york'\n",
    "    'from' -> 'angeles' | 'york'\n",
    "    'angeles' -> 'los'\n",
    "    'york' -> 'new'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "atis_grammar = nltk.DependencyGrammar.fromstring(atis_rules)\n",
    "atis_parser = nltk.ProjectiveDependencyParser(atis_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(show (flights from (to (york new) (angeles los))))\n",
      "The ROOT is 'show'\n",
      "(show (flights (from (york new)) (to (angeles los))))\n",
      "The ROOT is 'show'\n"
     ]
    }
   ],
   "source": [
    "atis_sent = \"show flights from new york to los angeles\"\n",
    "\n",
    "for tree in atis_parser.parse(atis_sent.split()):\n",
    "    print(tree)\n",
    "    # print ROOT node\n",
    "    print(\"The ROOT is '{}'\".format(tree.label()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise #2\n",
    "\n",
    "Write a function that given a dependency graph, for each token (word), produces list of words from it to ROOT.\n",
    "\n",
    "(Construct normal `dict` for simplicity first.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading treebank\n",
    "# nltk.download('dependency_treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import dependency_treebank\n",
    "\n",
    "graph = dependency_treebank.parsed_sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Pierre', 'Vinken', 'will'], ['Vinken', 'will'], ['will'], [',', 'Vinken', 'will'], ['61', 'years', 'old', 'Vinken', 'will'], ['years', 'old', 'Vinken', 'will'], ['old', 'Vinken', 'will'], [',', 'Vinken', 'will'], ['join', 'will'], ['the', 'board', 'join', 'will'], ['board', 'join', 'will'], ['as', 'join', 'will'], ['a', 'director', 'as', 'join', 'will'], ['director', 'as', 'join', 'will'], ['nonexecutive', 'director', 'as', 'join', 'will'], ['Nov.', 'join', 'will'], ['29', 'Nov.', 'join', 'will'], ['.', 'will']]\n"
     ]
    }
   ],
   "source": [
    "paths = []\n",
    "for k, n in graph.nodes.items():\n",
    "    if k != 0:\n",
    "        head = n.get(\"head\")\n",
    "        path = [n.get(\"word\")]\n",
    "        while head != 0:\n",
    "            path.append(graph.nodes[head].get(\"word\"))\n",
    "            head = graph.nodes[head].get(\"head\")\n",
    "        paths.append(path)\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise #3\n",
    "- Train `arc-standard` and `arc-eager` transition parsers on the same portion (slightly bigger than 100, otherwise it takes a lot of time)\n",
    "- Evaluate both of them comparing the attachment scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.transitionparser import TransitionParser\n",
    "from nltk.parse import DependencyEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of training examples : 100\n",
      " Number of valid (projective) examples : 100\n",
      "[LibSVM]..*.*\n",
      "optimization finished, #iter = 3974\n",
      "obj = -112.068968, rho = 0.659737\n",
      "nSV = 1586, nBSV = 47\n",
      "Total nSV = 1586\n",
      "..*..*\n",
      "optimization finished, #iter = 4016\n",
      "obj = -112.886225, rho = 0.676413\n",
      "nSV = 1596, nBSV = 40\n",
      "Total nSV = 1596\n",
      "..*..*\n",
      "optimization finished, #iter = 4131\n",
      "obj = -113.235237, rho = 0.667563\n",
      "nSV = 1614, nBSV = 40\n",
      "Total nSV = 1614\n",
      "..*.*\n",
      "optimization finished, #iter = 3996\n",
      "obj = -113.438075, rho = 0.656064\n",
      "nSV = 1594, nBSV = 45\n",
      "Total nSV = 1594\n",
      "..*..*\n",
      "optimization finished, #iter = 4013\n",
      "obj = -113.001936, rho = 0.670070\n",
      "nSV = 1602, nBSV = 37\n",
      "Total nSV = 1602\n",
      "...*.*\n",
      "optimization finished, #iter = 4902\n",
      "obj = -131.080714, rho = -0.664586\n",
      "nSV = 1856, nBSV = 50\n",
      "..*..*\n",
      "optimization finished, #iter = 4200\n",
      "obj = -179.561648, rho = 0.444999\n",
      "nSV = 1760, nBSV = 153\n",
      "Total nSV = 1760\n",
      "...*.*\n",
      "optimization finished, #iter = 4263\n",
      "obj = -177.253723, rho = 0.465356\n",
      "nSV = 1756, nBSV = 144\n",
      "Total nSV = 1756\n",
      "..*..*\n",
      "optimization finished, #iter = 4106\n",
      "obj = -183.013141, rho = 0.431026\n",
      "nSV = 1738, nBSV = 167\n",
      "Total nSV = 1738\n",
      "..*..*\n",
      "optimization finished, #iter = 4151\n",
      "obj = -183.164164, rho = 0.441670\n",
      "nSV = 1769, nBSV = 163\n",
      "Total nSV = 1769\n",
      "..*..*\n",
      "optimization finished, #iter = 4267\n",
      "obj = -183.064391, rho = 0.422494\n",
      "nSV = 1777, nBSV = 166\n",
      "Total nSV = 1777\n",
      "...*..*\n",
      "optimization finished, #iter = 5001\n",
      "obj = -216.029956, rho = -0.387953\n",
      "nSV = 2058, nBSV = 215\n",
      ".*.*\n",
      "optimization finished, #iter = 2621\n",
      "obj = -72.901505, rho = -0.250053\n",
      "nSV = 1197, nBSV = 21\n",
      "Total nSV = 1197\n",
      ".*.*\n",
      "optimization finished, #iter = 2607\n",
      "obj = -74.643573, rho = -0.227360\n",
      "nSV = 1192, nBSV = 24\n",
      "Total nSV = 1192\n",
      ".*.*\n",
      "optimization finished, #iter = 2618\n",
      "obj = -73.389634, rho = -0.253291\n",
      "nSV = 1189, nBSV = 21\n",
      "Total nSV = 1189\n",
      ".*.*\n",
      "optimization finished, #iter = 2569\n",
      "obj = -73.468517, rho = -0.250833\n",
      "nSV = 1197, nBSV = 24\n",
      "Total nSV = 1197\n",
      ".*.*\n",
      "optimization finished, #iter = 2650\n",
      "obj = -73.243803, rho = -0.225741\n",
      "nSV = 1215, nBSV = 21\n",
      "Total nSV = 1215\n",
      "..*.*\n",
      "optimization finished, #iter = 3081\n",
      "obj = -83.670783, rho = 0.240401\n",
      "nSV = 1384, nBSV = 27\n",
      "Total nSV = 3454\n"
     ]
    }
   ],
   "source": [
    "# train arc-standard parser\n",
    "tp_as = TransitionParser('arc-standard')\n",
    "tp_as.train(dependency_treebank.parsed_sents()[:100], 'tp_as.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of training examples : 100\n",
      " Number of valid (projective) examples : 100\n",
      "[LibSVM].*.*\n",
      "optimization finished, #iter = 2902\n",
      "obj = -83.125594, rho = 0.170151\n",
      "nSV = 1242, nBSV = 16\n",
      "Total nSV = 1242\n",
      "..*\n",
      "optimization finished, #iter = 2988\n",
      "obj = -79.786430, rho = 0.162975\n",
      "nSV = 1228, nBSV = 9\n",
      "Total nSV = 1228\n",
      "..*\n",
      "optimization finished, #iter = 2987\n",
      "obj = -85.541882, rho = 0.179176\n",
      "nSV = 1229, nBSV = 19\n",
      "Total nSV = 1229\n",
      "..*\n",
      "optimization finished, #iter = 2981\n",
      "obj = -82.226606, rho = 0.155186\n",
      "nSV = 1237, nBSV = 14\n",
      "Total nSV = 1237\n",
      "..*\n",
      "optimization finished, #iter = 2943\n",
      "obj = -80.967025, rho = 0.155182\n",
      "nSV = 1227, nBSV = 16\n",
      "Total nSV = 1227\n",
      "..*.*\n",
      "optimization finished, #iter = 3592\n",
      "obj = -95.632963, rho = -0.167307\n",
      "nSV = 1438, nBSV = 18\n",
      ".*.*\n",
      "optimization finished, #iter = 2789\n",
      "obj = -97.913165, rho = -0.068342\n",
      "nSV = 1378, nBSV = 28\n",
      "Total nSV = 1378\n",
      "..*\n",
      "optimization finished, #iter = 2906\n",
      "obj = -100.231027, rho = -0.065426\n",
      "nSV = 1388, nBSV = 36\n",
      "Total nSV = 1388\n",
      ".*.*\n",
      "optimization finished, #iter = 2844\n",
      "obj = -96.082972, rho = -0.068786\n",
      "nSV = 1371, nBSV = 28\n",
      "Total nSV = 1371\n",
      "..*\n",
      "optimization finished, #iter = 2857\n",
      "obj = -95.135934, rho = -0.066012\n",
      "nSV = 1383, nBSV = 26\n",
      "Total nSV = 1383\n",
      "..*\n",
      "optimization finished, #iter = 2935\n",
      "obj = -96.991003, rho = -0.064842\n",
      "nSV = 1402, nBSV = 33\n",
      "Total nSV = 1402\n",
      "..*.*\n",
      "optimization finished, #iter = 3453\n",
      "obj = -114.539651, rho = 0.076624\n",
      "nSV = 1653, nBSV = 39\n",
      ".*.*\n",
      "optimization finished, #iter = 2433\n",
      "obj = -60.711876, rho = 0.256715\n",
      "nSV = 1115, nBSV = 10\n",
      "Total nSV = 1115\n",
      ".*.*\n",
      "optimization finished, #iter = 2310\n",
      "obj = -63.287356, rho = 0.238785\n",
      "nSV = 1098, nBSV = 4\n",
      "Total nSV = 1098\n",
      ".*.*\n",
      "optimization finished, #iter = 2296\n",
      "obj = -60.065551, rho = 0.250904\n",
      "nSV = 1097, nBSV = 10\n",
      "Total nSV = 1097\n",
      ".*.*\n",
      "optimization finished, #iter = 2443\n",
      "obj = -61.622106, rho = 0.228854\n",
      "nSV = 1117, nBSV = 8\n",
      "Total nSV = 1117\n",
      ".*.*\n",
      "optimization finished, #iter = 2461\n",
      "obj = -63.445052, rho = 0.245583\n",
      "nSV = 1095, nBSV = 9\n",
      "Total nSV = 1095\n",
      ".*.*\n",
      "optimization finished, #iter = 2849\n",
      "obj = -71.318405, rho = -0.246462\n",
      "nSV = 1283, nBSV = 10\n",
      "..*.*\n",
      "optimization finished, #iter = 3158\n",
      "obj = -83.452588, rho = -0.210302\n",
      "nSV = 1312, nBSV = 20\n",
      "Total nSV = 1312\n",
      "..*.*\n",
      "optimization finished, #iter = 3145\n",
      "obj = -83.703257, rho = -0.208069\n",
      "nSV = 1300, nBSV = 15\n",
      "Total nSV = 1300\n",
      "..*.*\n",
      "optimization finished, #iter = 3081\n",
      "obj = -82.074633, rho = -0.203545\n",
      "nSV = 1293, nBSV = 16\n",
      "Total nSV = 1293\n",
      "..*.*\n",
      "optimization finished, #iter = 3125\n",
      "obj = -84.363207, rho = -0.209410\n",
      "nSV = 1296, nBSV = 21\n",
      "Total nSV = 1296\n",
      "..*.*\n",
      "optimization finished, #iter = 3070\n",
      "obj = -83.430087, rho = -0.202328\n",
      "nSV = 1290, nBSV = 17\n",
      "Total nSV = 1290\n",
      "..*.*\n",
      "optimization finished, #iter = 3819\n",
      "obj = -96.436134, rho = 0.215477\n",
      "nSV = 1532, nBSV = 19\n",
      ".*.*\n",
      "optimization finished, #iter = 2268\n",
      "obj = -70.386442, rho = 0.147259\n",
      "nSV = 1018, nBSV = 22\n",
      "Total nSV = 1018\n",
      ".*.*\n",
      "optimization finished, #iter = 2324\n",
      "obj = -76.649815, rho = 0.135456\n",
      "nSV = 1039, nBSV = 31\n",
      "Total nSV = 1039\n",
      ".*.*\n",
      "optimization finished, #iter = 2281\n",
      "obj = -75.247743, rho = 0.131685\n",
      "nSV = 1044, nBSV = 31\n",
      "Total nSV = 1044\n",
      ".*.*\n",
      "optimization finished, #iter = 2328\n",
      "obj = -73.518521, rho = 0.107243\n",
      "nSV = 1026, nBSV = 24\n",
      "Total nSV = 1026\n",
      ".*.*\n",
      "optimization finished, #iter = 2295\n",
      "obj = -74.366963, rho = 0.112669\n",
      "nSV = 1019, nBSV = 27\n",
      "Total nSV = 1019\n",
      ".*.*\n",
      "optimization finished, #iter = 2811\n",
      "obj = -87.761306, rho = -0.130526\n",
      "nSV = 1208, nBSV = 38\n",
      ".*.*\n",
      "optimization finished, #iter = 2844\n",
      "obj = -97.649268, rho = 0.396554\n",
      "nSV = 1146, nBSV = 36\n",
      "Total nSV = 1146\n",
      ".*.*\n",
      "optimization finished, #iter = 2770\n",
      "obj = -97.932390, rho = 0.379145\n",
      "nSV = 1108, nBSV = 38\n",
      "Total nSV = 1108\n",
      ".*.*\n",
      "optimization finished, #iter = 2690\n",
      "obj = -97.648548, rho = 0.373321\n",
      "nSV = 1119, nBSV = 41\n",
      "Total nSV = 1119\n",
      ".*.*\n",
      "optimization finished, #iter = 2769\n",
      "obj = -101.832530, rho = 0.425219\n",
      "nSV = 1123, nBSV = 44\n",
      "Total nSV = 1123\n",
      ".*.*\n",
      "optimization finished, #iter = 2722\n",
      "obj = -99.104495, rho = 0.392346\n",
      "nSV = 1097, nBSV = 45\n",
      "Total nSV = 1097\n",
      "..*.*\n",
      "optimization finished, #iter = 3398\n",
      "obj = -117.326447, rho = -0.415856\n",
      "nSV = 1316, nBSV = 56\n",
      "Total nSV = 3642\n"
     ]
    }
   ],
   "source": [
    "# train arc-eager parser\n",
    "tp_ae = TransitionParser('arc-eager')\n",
    "tp_ae.train(dependency_treebank.parsed_sents()[:100], 'tp_ae.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation\n",
    "get parses from each models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "as_parses = tp_as.parse(dependency_treebank.parsed_sents()[-10:], 'tp_as.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7791666666666667, 0.7791666666666667)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_as = DependencyEvaluator(as_parses, dependency_treebank.parsed_sents()[-10:])\n",
    "de_as.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_parses = tp_ae.parse(dependency_treebank.parsed_sents()[-10:], 'tp_ae.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7708333333333334, 0.7708333333333334)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_ae = DependencyEvaluator(ae_parses, dependency_treebank.parsed_sents()[-10:])\n",
    "de_ae.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Lab Exercise Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write functions to convert `spacy` and `stanza` dependency parses into `NLTK`'s [`DependencyGraph`](https://www.nltk.org/_modules/nltk/parse/dependencygraph.html)\n",
    "    - make use of `load` and [Malt-Tab format](https://cl.lingfil.uu.se/~nivre/research/MaltXML.html)\n",
    "- Parse 100 last sentences from dependency treebank using `spacy` and `stanza`\n",
    "- Evaluate the parses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "The only difference between spacy and stanza is in the coversion of the output to proper format.\n",
    "\n",
    "As an example we will do spacy.\n",
    "\n",
    "Also, we are going to igore dependency relation mapping; thus, our labeled attachement score will be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import en_core_web_sm\n",
    "spacy_nlp = en_core_web_sm.load()\n",
    "\n",
    "# to use white space tokenization (generally a bad idea for unknown data)\n",
    "spacy_nlp.tokenizer = Tokenizer(spacy_nlp.vocab)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import dependency_treebank\n",
    "# get last 100 sentences\n",
    "\n",
    "test_fold = dependency_treebank.parsed_sents()[-100:]\n",
    "print(len(test_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to extract sentence text from parses\n",
    "def graph2sent(graph):\n",
    "    sent = [(int(k), n[\"word\"]) for k, n in graph.nodes.items()]\n",
    "    sent = sorted(sent, key=lambda x: x[0])\n",
    "    sent = \" \".join([w for _, w in sent[1:]])\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extact sentence text from parses\n",
    "test_text = [graph2sent(g) for g in test_fold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to covert spacy doc to MaltTab fields: token, postag, head ID, relation\n",
    "# we will ignore mapping from spacy relations to NLTK relations, it could be done using dict\n",
    "def spacy2malt(text):\n",
    "    doc = spacy_nlp(text)\n",
    "    # (0 if t.head.i == t.i else t.head.i) will use 0 as a head ID for tokens that are ROOT (by default it's token ID)\n",
    "    # also need to shift ids by 1 (i.e. start from 1)\n",
    "    return [(t.text, t.pos_, (0 if t.head.i == t.i else t.head.i + 1), t.dep_) for t in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Colorless', 'ADJ', 3, 'amod'), ('green', 'ADJ', 3, 'amod'), ('ideas', 'NOUN', 4, 'nsubj'), ('sleep', 'VERB', 0, 'ROOT'), ('fusiously', 'ADV', 4, 'advmod'), ('.', 'PUNCT', 4, 'punct')]\n"
     ]
    }
   ],
   "source": [
    "# let's test the function\n",
    "print(spacy2malt(\"Colorless green ideas sleep fusiously .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's parse and dump the test set to file\n",
    "with open(\"test_parses.txt\", \"w\") as fh:\n",
    "    for i, sent in enumerate(test_text):\n",
    "        for tok in spacy2malt(sent):\n",
    "            fh.write(\"\\t\".join([str(p) for p in tok]) + \"\\n\")\n",
    "        if i != len(test_text) - 1:\n",
    "            fh.write(\"\\n\")  # sentences are new line separated, but we don't need 2 newlines at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load parses as graphs\n",
    "from nltk.parse.dependencygraph import DependencyGraph\n",
    "test_parses = DependencyGraph.load(\"test_parses.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.696276357110812)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation\n",
    "# notice that LAS is 0 due to missing mapping\n",
    "from nltk.parse import DependencyEvaluator\n",
    "de = DependencyEvaluator(test_parses, test_fold)\n",
    "de.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
