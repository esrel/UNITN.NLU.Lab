{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling using OpenGRM: Solutions\n",
    "\n",
    "- Natural Language Understanding\n",
    "- Evgeny A. Stepanov\n",
    "- stepanov.evgeny.a@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers Lecture on __Sequence and Language Modeling__ using OpenFST and OpenGRM NGram Library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Requirements__\n",
    "\n",
    "- [OpenFST](http://www.openfst.org/twiki/bin/view/FST/WebHome)\n",
    "- [OpenGRM](http://www.opengrm.org/twiki/bin/view/GRM/NGramLibrary)\n",
    "- [NL2SparQL4NLU](https://github.com/esrel/NL2SparQL4NLU) dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "- Read Tool Manuals (for available tools and their options; they start with `far`, `ngram` and `fst`)\n",
    "\n",
    "- Preprocess training and test sets using [corpus and lexicon preprocessing functions](corpus_pp_python.ipynb) to:\n",
    "    - add sentence begin & end tags\n",
    "    - handle unknown words (e.g. frequency cut-off)\n",
    "    \n",
    "- Convert corpus and lexicon to OpenGRM format\n",
    "\n",
    "- Train different language models (LM) on the training set of `NL2SparQL4NLU` using OpenGRM NGram Library\n",
    "\n",
    "    - vary n-gram order\n",
    "    - vary smoothing\n",
    "    - compute LM perplexity on the test set\n",
    "\n",
    "- Report order & smoothing method with lowest perplexity\n",
    "\n",
    "- Compute score for utterances in the test set\n",
    "- Compare score to the ones computed manually\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenGRM Corpus Pre-processing & Lexicon Generation\n",
    "\n",
    "OpenGRM requires lexicon and corpus to be in a specific formats, and provides tools for:\n",
    "\n",
    "- Automatic lexicon extraction from corpus (`ngramsymbols`)\n",
    "- Compilation of text into FAR\n",
    "    - option to replace OOV (`farcompilestrings --unknown_symbol`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "fname=NL2SparQL4NLU\n",
    "trn=$fname.trn.data\n",
    "tst=$fname.tst.data\n",
    "\n",
    "# create lexicon in the required format\n",
    "ngramsymbols < $trn > $fname.lex\n",
    "\n",
    "# complile training set into FAR, replacing OOV with '<unk>'\n",
    "# already done externally, but in case you have modified lexicon, this is the command\n",
    "farcompilestrings --unknown_symbol=\"<unk>\" --symbols=$fname.lex -keep_symbols=1 $trn > $fname.trn.far \n",
    "\n",
    "# compile test set into FAR, replacing OOV with '<unk>'\n",
    "farcompilestrings --unknown_symbol=\"<unk>\" --symbols=$fname.lex -keep_symbols=1 $tst > $fname.tst.far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ngram Counting & Language Model Training\n",
    "\n",
    "- Modify `--method` and `--order` to vary *smoothing* and *ngram order*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1084 sentences, 9285 words, 0 OOVs\n",
      "logprob(base 10)= -10556.3;  perplexity = 10.4247\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: OOV probability = 0; OOVs will be ignored in perplexity calculation\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "fname=NL2SparQL4NLU\n",
    "\n",
    "# counts ngrams\n",
    "ngramcount --order=3 $fname.trn.far > $fname.trn.counts\n",
    "# make LM \n",
    "ngrammake --method=katz $fname.trn.counts > $fname.lm\n",
    "\n",
    "# compute perplexity on the test set (ignore warnings)\n",
    "ngramperplexity $fname.lm $fname.tst.far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "fname=NL2SparQL4NLU\n",
    "# apply LM\n",
    "ngramapply $fname.lm $fname.tst.far $fname.tst.out.far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing Scored Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> star of <unk> </s>\t11.3299\n",
      "<s> who is in the movie the campaign </s>\t13.7932\n",
      "<s> list the cast of the movie the campaign </s>\t19.1071\n",
      "<s> who was in twilight </s>\t13.3456\n",
      "<s> who is in <unk> </s>\t10.052\n",
      "<s> actor from lost </s>\t19.9336\n",
      "<s> who played in the movie rocky </s>\t24.6591\n",
      "<s> who played in the movie captain america </s>\t19.2894\n",
      "<s> cast and crew for in july </s>\t11.7449\n",
      "<s> who is in movie in july </s>\t12.5787\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "fname=NL2SparQL4NLU\n",
    "# print strings with weights\n",
    "farprintstrings --print_weight $fname.tst.out.far | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
