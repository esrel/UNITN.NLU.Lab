{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Solutions for Sequence Labeling: Part-of-Speech Tagging\n",
    "- Evgeny A. Stepanov\n",
    "- stepanov.evgeny.a@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lab Exercise: Comparative Evaluation of NLTK Taggers\n",
    "\n",
    "Experiment with different taggers provided in NLTK (e.g. NgramTagger)\n",
    "\n",
    "- Train and evaluate taggers provided by NLTK\n",
    "    - experiment with different tagger parameters\n",
    "    - some of them have *cut-off*\n",
    "\n",
    "- For each report evaluation accuracy\n",
    "\n",
    "- Evaluate `spacy` POS-tags on the same test set\n",
    "    - create mapping from spacy to NLTK POS-tags\n",
    "    - convert output to the required format (see format above)\n",
    "        - flatten into a list\n",
    "    - evaluate using `accuracy` from `nltk.metrics` \n",
    "        - [link](https://www.nltk.org/_modules/nltk/metrics/scores.html#accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Solution\n",
    "The challenging part of the exercise is evaluation of `spacy`, since there are tokenization differences to be addressed. The easiest solution is to disable `spacy` tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import en_core_web_sm\n",
    "spacy_nlp = en_core_web_sm.load()\n",
    "\n",
    "# to use white space tokenization (generally a bad idea for unknown data)\n",
    "spacy_nlp.tokenizer = Tokenizer(spacy_nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping from `spacy` to `NLTK` POS-tags. It's sufficient to deal with the different tags, since the rest is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    \"PROPN\": \"NOUN\", \"INTJ\": \"NOUN\", \n",
    "    \"AUX\": \"VERB\", \n",
    "    \"PART\": \"PRT\",\n",
    "    \"CCONJ\": \"CONJ\", \"SCONJ\": \"CONJ\",\n",
    "    \"SYM\": \"X\",\n",
    "    \"PUNCT\": \".\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DATA SPLIT__ (copied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 3914; Train: 3132; Test: 782\n"
     ]
    }
   ],
   "source": [
    "# Prepare Training & Test Splits as 80%/20%\n",
    "import math\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "total_size = len(treebank.tagged_sents())\n",
    "train_indx = math.ceil(total_size * 0.8)\n",
    "trn_data = treebank.tagged_sents(tagset='universal')[:train_indx]\n",
    "tst_data = treebank.tagged_sents(tagset='universal')[train_indx:]\n",
    "\n",
    "print(\"Total: {}; Train: {}; Test: {}\".format(total_size, len(trn_data), len(tst_data)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TAGGING with SPACY__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DET'), ('discount', 'NOUN'), ('rate', 'NOUN'), ('on', 'ADP'), ('three-month', 'ADJ'), ('Treasury', 'NOUN'), ('bills', 'NOUN'), ('was', 'VERB'), ('essentially', 'ADV'), ('unchanged', 'ADJ'), ('at', 'ADP'), ('7.79', 'NUM'), ('%', 'NOUN'), (',', '.'), ('while', 'ADP'), ('the', 'DET'), ('rate', 'NOUN'), ('on', 'ADP'), ('six-month', 'ADJ'), ('bills', 'NOUN'), ('was', 'VERB'), ('slightly', 'ADV'), ('lower', 'ADJ'), ('at', 'ADP'), ('7.52', 'NUM'), ('%', 'NOUN'), ('compared', 'VERB'), ('with', 'ADP'), ('7.60', 'NUM'), ('%', 'NOUN'), ('Tuesday', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(tst_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_result = []\n",
    "for sent in treebank.sents()[train_indx:]:\n",
    "    sent_doc = spacy_nlp(\" \".join(sent))\n",
    "    # we use mapping here to replace spacy tags with NLTK tags\n",
    "    spacy_result.append([(t.text, mapping.get(t.pos_, t.pos_)) for t in sent_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__EVALUATION__\n",
    "1. flatten both hypotheses and references\n",
    "2. evaluate using `accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20015\n",
      "20015\n"
     ]
    }
   ],
   "source": [
    "flat_ref = [element for sublist in tst_data for element in sublist]\n",
    "flat_hyp = [element for sublist in spacy_result for element in sublist]\n",
    "print(len(flat_ref))\n",
    "print(len(flat_hyp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8596552585560829"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics import accuracy\n",
    "accuracy(flat_ref, flat_hyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance is quite low, given the task. \n",
    "The reason for the difference, besides the tag differences that we might have missed (e.g. `$` is `.` in referece, but tagged as `X` by `scpay), are \n",
    "\n",
    "- the \"special\" replacements in the `treebank`, such as `-LRB-` and `-RRB-` instead of `(` and `)`, and alike. \n",
    "\n",
    "- Additional complexity comes from the \"traces\" that are present in the sentences: \"words\" that start with `*` and `0` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do the replacements w.r.t words\n",
    "word_mapping = {\"$\": \".\", \"-LRB-\": \".\", \"-RRB-\": \".\", \"0\": \"X\"}\n",
    "\n",
    "flat_hyp_pp = [(w, word_mapping.get(w, t)) for w, t in flat_hyp]\n",
    "# replace traces\n",
    "flat_hyp_pp = [(w, (\"X\" if w.startswith('*') else t)) for w, t in flat_hyp_pp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9408443667249563"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(flat_ref, flat_hyp_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
