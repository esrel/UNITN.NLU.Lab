{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Solutions for Sequence Labeling: Shallow Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evgeny A. Stepanov\n",
    "- stepanov.evgeny.a@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Named Entity Recognition with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.2. Training NLTK Taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to /Users/eas/nltk_data...\n",
      "[nltk_data]   Package conll2002 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('conll2002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2002\n",
    "import nltk.tag.hmm as hmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Segmentation \n",
    "Train a tagger to perform *segmentation* of input sentences into constituents\n",
    "- Strip concept information from output labels (i.e. keep only IOB-prefix)\n",
    "- Train tagger to predict segmentation labels\n",
    "- Evaluate segmentation performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function to stip concept information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tag(tag):\n",
    "    parts = tag.split(\"-\")\n",
    "    return tag if len(parts) == 1 else parts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_sents_seg = [[(text, split_tag(iob)) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.train')]\n",
    "testa_sents_seg = [[(text, split_tag(iob)) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.testa')]\n",
    "testb_sents_seg = [[(text, split_tag(iob)) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.testb')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4297\n"
     ]
    }
   ],
   "source": [
    "# re-define again to avoid parameters being taken from other models\n",
    "hmm_seg_model = hmm.HiddenMarkovModelTrainer()\n",
    "hmm_seg = hmm_seg_model.train(train_sents_seg)\n",
    "    \n",
    "# evaluation\n",
    "accuracy = hmm_seg.accuracy(testa_sents_seg)\n",
    "\n",
    "print(\"Accuracy: {:6.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### CoNLL Eval Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# to import conll\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../src/'))\n",
    "\n",
    "from conll import evaluate\n",
    "# for nice tables\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Segmentation Evaluation\n",
    "we need to retrain or post-process the output of segmentation tagger to match the required input format.\n",
    "\n",
    "`PREFIX-TAG`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ne(tag, ne_tag=\"NE\"):\n",
    "    return tag if tag == \"O\" else \"-\".join([tag, ne_tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag\n",
    "hyp_a_seg = [hmm_seg.tag(s) for s in conll2002.sents('esp.testa')]\n",
    "hyp_b_seg = [hmm_seg.tag(s) for s in conll2002.sents('esp.testb')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post-process references\n",
    "ref_a = [[(text, add_ne(iob)) for text, iob in sent] for sent in testa_sents_seg]\n",
    "ref_b = [[(text, add_ne(iob)) for text, iob in sent] for sent in testb_sents_seg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post-process hypotheses\n",
    "hyp_a = [[(text, add_ne(iob)) for text, iob in sent] for sent in hyp_a_seg]\n",
    "hyp_b = [[(text, add_ne(iob)) for text, iob in sent] for sent in hyp_b_seg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_a = evaluate(ref_a, hyp_a)\n",
    "res_b = evaluate(ref_b, hyp_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NE</th>\n",
       "      <td>0.087</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.153</td>\n",
       "      <td>4352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>0.087</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.153</td>\n",
       "      <td>4352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           p      r      f     s\n",
       "NE     0.087  0.679  0.153  4352\n",
       "total  0.087  0.679  0.153  4352"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_tbl_a = pd.DataFrame().from_dict(res_a, orient='index')\n",
    "pd_tbl_a.round(decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NE</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.144</td>\n",
       "      <td>3559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.144</td>\n",
       "      <td>3559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          p      r      f     s\n",
       "NE     0.08  0.717  0.144  3559\n",
       "total  0.08  0.717  0.144  3559"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_tbl_b = pd.DataFrame().from_dict(res_b, orient='index')\n",
    "pd_tbl_b.round(decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Re-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def split_tag_ne(tag, ne_tag=\"NE\"):\n",
    "    parts = tag.split(\"-\")\n",
    "    return tag if len(parts) == 1 else \"-\".join([parts[0], ne_tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents_seg_ne = [[(text, split_tag_ne(iob)) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.train')]\n",
    "testa_sents_seg_ne = [[(text, split_tag_ne(iob)) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.testa')]\n",
    "testb_sents_seg_ne = [[(text, split_tag_ne(iob)) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.testb')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy A: 0.4297\n",
      "Accuracy B: 0.4432\n"
     ]
    }
   ],
   "source": [
    "hmm_seg_ne_model = hmm.HiddenMarkovModelTrainer()\n",
    "hmm_seg_ne = hmm_seg_ne_model.train(train_sents_seg_ne)\n",
    "    \n",
    "# evaluation\n",
    "acca = hmm_seg_ne.accuracy(testa_sents_seg_ne)\n",
    "accb = hmm_seg_ne.accuracy(testb_sents_seg_ne)\n",
    "print(\"Accuracy A: {:6.4f}\".format(acca))\n",
    "print(\"Accuracy B: {:6.4f}\".format(accb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting hypotheses\n",
    "hyp_a_ne = [hmm_seg_ne.tag(s) for s in conll2002.sents('esp.testa')]\n",
    "hyp_b_ne = [hmm_seg_ne.tag(s) for s in conll2002.sents('esp.testb')]\n",
    "\n",
    "res_a_ne = evaluate(testa_sents_seg_ne, hyp_a_ne)\n",
    "res_b_ne = evaluate(testb_sents_seg_ne, hyp_b_ne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NE</th>\n",
       "      <td>0.087</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.153</td>\n",
       "      <td>4352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>0.087</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.153</td>\n",
       "      <td>4352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           p      r      f     s\n",
       "NE     0.087  0.679  0.153  4352\n",
       "total  0.087  0.679  0.153  4352"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_tbl_a = pd.DataFrame().from_dict(res_a_ne, orient='index')\n",
    "pd_tbl_a.round(decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NE</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.144</td>\n",
       "      <td>3559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.144</td>\n",
       "      <td>3559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          p      r      f     s\n",
       "NE     0.08  0.717  0.144  3559\n",
       "total  0.08  0.717  0.144  3559"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_tbl_b = pd.DataFrame().from_dict(res_b_ne, orient='index')\n",
    "pd_tbl_b.round(decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. SpaCy Token Features\n",
    "\n",
    "[spaCy](https://spacy.io/) provides a convenient way to augment our feature set with common features using in Natural Language Processing. \n",
    "\n",
    "The list of provided token-level features is available [here](https://spacy.io/api/token#attributes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- add suffix features to the model and report performances\n",
    "- try the feature template from the tutorial on CoNLL dataset\n",
    "- increase the feature window (number of previous and next token) to:\n",
    "    - `[-1, +1]`\n",
    "    - `[-2, +2]`\n",
    "- learn & experiment with model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify `sent2features` function to make use of spaCy features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.2.0/es_core_news_sm-3.2.0-py3-none-any.whl (14.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.0 MB 14.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /opt/miniconda3/envs/unitn/lib/python3.9/site-packages (from es-core-news-sm==3.2.0) (3.2.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/miniconda3/envs/unitn/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: setuptools in /opt/miniconda3/envs/unitn/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (58.0.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/miniconda3/envs/unitn/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/miniconda3/envs/unitn/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (1.22.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/miniconda3/envs/unitn/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/miniconda3/envs/unitn/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /opt/miniconda3/envs/unitn/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/envs/unitn/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (21.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/miniconda3/envs/unitn/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/miniconda3/envs/unitn/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /opt/miniconda3/envs/unitn/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/miniconda3/envs/unitn/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/miniconda3/envs/unitn/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /opt/miniconda3/envs/unitn/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/miniconda3/envs/unitn/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/miniconda3/envs/unitn/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (2.27.1)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/unitn/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (3.0.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/miniconda3/envs/unitn/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/miniconda3/envs/unitn/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/miniconda3/envs/unitn/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/miniconda3/envs/unitn/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /opt/miniconda3/envs/unitn/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/envs/unitn/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (4.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/miniconda3/envs/unitn/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/unitn/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/miniconda3/envs/unitn/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (2.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/unitn/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (3.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/miniconda3/envs/unitn/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/unitn/lib/python3.9/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (2.0.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "# get Spanish model of spacy\n",
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = [[(text, iob) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.train')]\n",
    "testa_sents = [[(text, iob) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.testa')]\n",
    "testb_sents = [[(text, iob) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.testb')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    return {'bias': 1.0, 'word.lower()': word.lower()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import es_core_news_sm\n",
    "nlp = es_core_news_sm.load()\n",
    "\n",
    "# nlp = spacy.load(\"es_core_news_sm\")\n",
    "nlp.tokenizer = Tokenizer(nlp.vocab)  # to use white space tokenization (generally a bad idea for unknown data)\n",
    "\n",
    "def sent2spacy_features(sent):\n",
    "    neg_window = 2 # number of previous tokens to featurize\n",
    "    pos_window = 2 # number of following tokens to featurize\n",
    "    spacy_sent = nlp(\" \".join(sent2tokens(sent)))\n",
    "    feats = []\n",
    "    # this replaces word 2 features\n",
    "    for token in spacy_sent:\n",
    "        token_feats = {\n",
    "            'bias': 1.0,\n",
    "            'word.lower()': token.lower_,\n",
    "            'pos': token.pos_,\n",
    "            'lemma': token.lemma_,\n",
    "            # adding this as part of the exercise \n",
    "            # (if word is less than suffix size, it will be used: e.g. suffix3 for 'we' is 'we')\n",
    "            'prefix1': token.text[:1],\n",
    "            'prefix2': token.text[:2],\n",
    "            'prefix3': token.text[:3],\n",
    "            'suffix1': token.text[-1:],\n",
    "            'suffix2': token.text[-2:],\n",
    "            'suffix3': token.text[-3:],\n",
    "            # some features from template\n",
    "            # https://spacy.io/api/token#attributes\n",
    "            'word.isupper()': token.is_upper,\n",
    "            'word.istitle()': token.is_title,\n",
    "            'word.isdigit()': token.is_digit,\n",
    "        }\n",
    "        # windowing: adding +/-2 with uniform features\n",
    "         # returns Span\n",
    "        prev_tokens = spacy_sent[(0 if (token.i - neg_window) < 0 else token.i - neg_window):token.i]\n",
    "        next_tokens = spacy_sent[(token.i + 1):(token.i + 1 + pos_window)]\n",
    "        \n",
    "        if len(prev_tokens) == 0:\n",
    "            token_feats['BOS'] = True\n",
    "        else:\n",
    "            for t in prev_tokens:\n",
    "                # unique identifier\n",
    "                str_id = str((t.i - token.i))\n",
    "                token_feats.update({\n",
    "                    f'{str_id}:word.lower()': t.lower_,\n",
    "                    f'{str_id}:word.istitle()': t.is_title,\n",
    "                    f'{str_id}:word.isupper()': t.is_upper,\n",
    "                    f'{str_id}:postag': t.pos_,\n",
    "            })\n",
    "                \n",
    "        if len(next_tokens) == 0:\n",
    "            token_feats['EOS'] = True\n",
    "        else:\n",
    "            for t in next_tokens:\n",
    "                # unique identifier\n",
    "                str_id = \"+\" + str((t.i - token.i))\n",
    "                token_feats.update({\n",
    "                    f'{str_id}:word.lower()': t.lower_,\n",
    "                    f'{str_id}:word.istitle()': t.is_title,\n",
    "                    f'{str_id}:word.isupper()': t.is_upper,\n",
    "                    f'{str_id}:postag': t.pos_,\n",
    "            })\n",
    "\n",
    "        feats.append(token_feats)\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feats = [sent2spacy_features(s) for s in train_sents]\n",
    "train_label = [sent2labels(s) for s in train_sents]\n",
    "testa_feats = [sent2spacy_features(s) for s in testa_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'bias': 1.0, 'word.lower()': 'melbourne', 'pos': 'PROPN', 'lemma': 'Melbourne', 'prefix1': 'M', 'prefix2': 'Me', 'prefix3': 'Mel', 'suffix1': 'e', 'suffix2': 'ne', 'suffix3': 'rne', 'word.isupper()': False, 'word.istitle()': True, 'word.isdigit()': False, 'BOS': True, '+1:word.lower()': '(', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'PUNCT', '+2:word.lower()': 'australia', '+2:word.istitle()': True, '+2:word.isupper()': False, '+2:postag': 'PROPN'}, {'bias': 1.0, 'word.lower()': '(', 'pos': 'PUNCT', 'lemma': '(', 'prefix1': '(', 'prefix2': '(', 'prefix3': '(', 'suffix1': '(', 'suffix2': '(', 'suffix3': '(', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'melbourne', '-1:word.istitle()': True, '-1:word.isupper()': False, '-1:postag': 'PROPN', '+1:word.lower()': 'australia', '+1:word.istitle()': True, '+1:word.isupper()': False, '+1:postag': 'PROPN', '+2:word.lower()': ')', '+2:word.istitle()': False, '+2:word.isupper()': False, '+2:postag': 'PUNCT'}, {'bias': 1.0, 'word.lower()': 'australia', 'pos': 'PROPN', 'lemma': 'Australia', 'prefix1': 'A', 'prefix2': 'Au', 'prefix3': 'Aus', 'suffix1': 'a', 'suffix2': 'ia', 'suffix3': 'lia', 'word.isupper()': False, 'word.istitle()': True, 'word.isdigit()': False, '-2:word.lower()': 'melbourne', '-2:word.istitle()': True, '-2:word.isupper()': False, '-2:postag': 'PROPN', '-1:word.lower()': '(', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'PUNCT', '+1:word.lower()': ')', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'PUNCT', '+2:word.lower()': ',', '+2:word.istitle()': False, '+2:word.isupper()': False, '+2:postag': 'PUNCT'}, {'bias': 1.0, 'word.lower()': ')', 'pos': 'PUNCT', 'lemma': ')', 'prefix1': ')', 'prefix2': ')', 'prefix3': ')', 'suffix1': ')', 'suffix2': ')', 'suffix3': ')', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-2:word.lower()': '(', '-2:word.istitle()': False, '-2:word.isupper()': False, '-2:postag': 'PUNCT', '-1:word.lower()': 'australia', '-1:word.istitle()': True, '-1:word.isupper()': False, '-1:postag': 'PROPN', '+1:word.lower()': ',', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'PUNCT', '+2:word.lower()': '25', '+2:word.istitle()': False, '+2:word.isupper()': False, '+2:postag': 'NUM'}, {'bias': 1.0, 'word.lower()': ',', 'pos': 'PUNCT', 'lemma': ',', 'prefix1': ',', 'prefix2': ',', 'prefix3': ',', 'suffix1': ',', 'suffix2': ',', 'suffix3': ',', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-2:word.lower()': 'australia', '-2:word.istitle()': True, '-2:word.isupper()': False, '-2:postag': 'PROPN', '-1:word.lower()': ')', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'PUNCT', '+1:word.lower()': '25', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'NUM', '+2:word.lower()': 'may', '+2:word.istitle()': False, '+2:word.isupper()': False, '+2:postag': 'NOUN'}, {'bias': 1.0, 'word.lower()': '25', 'pos': 'NUM', 'lemma': '25', 'prefix1': '2', 'prefix2': '25', 'prefix3': '25', 'suffix1': '5', 'suffix2': '25', 'suffix3': '25', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': True, '-2:word.lower()': ')', '-2:word.istitle()': False, '-2:word.isupper()': False, '-2:postag': 'PUNCT', '-1:word.lower()': ',', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'PUNCT', '+1:word.lower()': 'may', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'NOUN', '+2:word.lower()': '(', '+2:word.istitle()': False, '+2:word.isupper()': False, '+2:postag': 'PUNCT'}, {'bias': 1.0, 'word.lower()': 'may', 'pos': 'NOUN', 'lemma': 'may', 'prefix1': 'm', 'prefix2': 'ma', 'prefix3': 'may', 'suffix1': 'y', 'suffix2': 'ay', 'suffix3': 'may', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-2:word.lower()': ',', '-2:word.istitle()': False, '-2:word.isupper()': False, '-2:postag': 'PUNCT', '-1:word.lower()': '25', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'NUM', '+1:word.lower()': '(', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'PUNCT', '+2:word.lower()': 'efe', '+2:word.istitle()': False, '+2:word.isupper()': True, '+2:postag': 'PROPN'}, {'bias': 1.0, 'word.lower()': '(', 'pos': 'PUNCT', 'lemma': '(', 'prefix1': '(', 'prefix2': '(', 'prefix3': '(', 'suffix1': '(', 'suffix2': '(', 'suffix3': '(', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-2:word.lower()': '25', '-2:word.istitle()': False, '-2:word.isupper()': False, '-2:postag': 'NUM', '-1:word.lower()': 'may', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'NOUN', '+1:word.lower()': 'efe', '+1:word.istitle()': False, '+1:word.isupper()': True, '+1:postag': 'PROPN', '+2:word.lower()': ')', '+2:word.istitle()': False, '+2:word.isupper()': False, '+2:postag': 'PUNCT'}, {'bias': 1.0, 'word.lower()': 'efe', 'pos': 'PROPN', 'lemma': 'EFE', 'prefix1': 'E', 'prefix2': 'EF', 'prefix3': 'EFE', 'suffix1': 'E', 'suffix2': 'FE', 'suffix3': 'EFE', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, '-2:word.lower()': 'may', '-2:word.istitle()': False, '-2:word.isupper()': False, '-2:postag': 'NOUN', '-1:word.lower()': '(', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'PUNCT', '+1:word.lower()': ')', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'PUNCT', '+2:word.lower()': '.', '+2:word.istitle()': False, '+2:word.isupper()': False, '+2:postag': 'PUNCT'}, {'bias': 1.0, 'word.lower()': ')', 'pos': 'PUNCT', 'lemma': ')', 'prefix1': ')', 'prefix2': ')', 'prefix3': ')', 'suffix1': ')', 'suffix2': ')', 'suffix3': ')', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-2:word.lower()': '(', '-2:word.istitle()': False, '-2:word.isupper()': False, '-2:postag': 'PUNCT', '-1:word.lower()': 'efe', '-1:word.istitle()': False, '-1:word.isupper()': True, '-1:postag': 'PROPN', '+1:word.lower()': '.', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'PUNCT'}, {'bias': 1.0, 'word.lower()': '.', 'pos': 'PUNCT', 'lemma': '.', 'prefix1': '.', 'prefix2': '.', 'prefix3': '.', 'suffix1': '.', 'suffix2': '.', 'suffix3': '.', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-2:word.lower()': 'efe', '-2:word.istitle()': False, '-2:word.isupper()': True, '-2:postag': 'PROPN', '-1:word.lower()': ')', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'PUNCT', 'EOS': True}]\n"
     ]
    }
   ],
   "source": [
    "print(train_feats[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_crfsuite import CRF\n",
    "\n",
    "crf = CRF(\n",
    "    algorithm='lbfgs', \n",
    "    c1=0.1, \n",
    "    c2=0.1, \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.9 s, sys: 419 ms, total: 37.3 s\n",
      "Wall time: 38.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# workaround for scikit-learn 1.0\n",
    "try:\n",
    "    crf.fit(train_feats, train_label)\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = crf.predict(testa_feats)\n",
    "\n",
    "hyp = [[(testa_feats[i][j], t) for j, t in enumerate(tokens)] for i, tokens in enumerate(pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0.775</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.759</td>\n",
       "      <td>1700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>0.666</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.726</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>0.562</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.499</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0.877</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.835</td>\n",
       "      <td>1222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>0.754</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.747</td>\n",
       "      <td>4352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           p      r      f     s\n",
       "ORG    0.775  0.745  0.759  1700\n",
       "LOC    0.666  0.798  0.726   985\n",
       "MISC   0.562  0.449  0.499   445\n",
       "PER    0.877  0.797  0.835  1222\n",
       "total  0.754  0.741  0.747  4352"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = evaluate(testa_sents, hyp)\n",
    "\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
